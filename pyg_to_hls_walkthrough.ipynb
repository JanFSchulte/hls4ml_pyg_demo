{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a1bc2776",
   "metadata": {},
   "source": [
    "# Installation\n",
    "Please follow the instructions on README.mk file for installing the necessary packages to run this notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24b78f5f",
   "metadata": {},
   "source": [
    "This walkthrough has few instructions. It's mainly just code to help the user to understand the pytorch geometric to hls4ml pipeline. If there's any confusion, please email me at yun79@purdue.edu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1f1907e",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e745ebbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-12 14:57:52.112984: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/lib:\n",
      "2022-10-12 14:57:52.113002: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "handler args: ('NodeBlock',)\n",
      "handler args: ('EdgeAggregate',)\n",
      "handler args: ('ResidualBlock',)\n",
      "handler args: ('NodeEncoder',)\n",
      "handler args: ('EdgeEncoder',)\n",
      "handler args: ('NodeEncoderBatchNorm1d',)\n",
      "handler args: ('EdgeEncoderBatchNorm1d',)\n",
      "handler args: ('MeanPool',)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import yaml\n",
    "import argparse\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from hls4ml.utils.config import config_from_pyg_model\n",
    "from hls4ml.converters import convert_from_pyg_model\n",
    "import hls4ml\n",
    "\n",
    "from collections import OrderedDict\n",
    "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score, mean_absolute_error, mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "\n",
    "# locals\n",
    "from utils.models.interaction_network_pyg import GENConvBig\n",
    "from model_wrappers import model_wrapper\n",
    "from utils.data.dataset_pyg import GraphDataset\n",
    "from utils.data.fix_graph_size import fix_graph_size\n",
    "import time\n",
    "import pickle as pkl\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb118bac",
   "metadata": {},
   "source": [
    "### PyTorch Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "24082fea",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "We intialize our custom pytorch geometric(pyg) model\n",
    "\"\"\"\n",
    "n_layers = 8\n",
    "torch_model = GENConvBig(\n",
    "    n_layers, \n",
    "    flow = \"source_to_target\",\n",
    "    out_channels = 128,\n",
    "    debugging = True\n",
    ").eval() # eval mode for bathnorm\n",
    "\"\"\"\n",
    "We obtain the state dict(trained parameters) from Siqi Miao, PhD student of Prof Pan Li\n",
    "\"\"\"\n",
    "state_dict = torch.load('./model.pt', map_location=\"cpu\")\n",
    "# state_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "602c6952",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch_model.node_encoder_norm.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bc6ffa38",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "load/transfer the state dict into our pyg model\n",
    "\"\"\"\n",
    "\n",
    "# print(type(torch_model.node_encoder.weight))\n",
    "# print(type(siqi_model_state_dict['model_state_dict']['node_encoder.weight']))\n",
    "torch_model.node_encoder.weight = nn.Parameter(state_dict['model_state_dict']['node_encoder.weight'])\n",
    "torch_model.node_encoder.bias = nn.Parameter(state_dict['model_state_dict']['node_encoder.bias'])\n",
    "torch_model.edge_encoder.weight = nn.Parameter(state_dict['model_state_dict']['edge_encoder.weight'])\n",
    "torch_model.edge_encoder.bias = nn.Parameter(state_dict['model_state_dict']['edge_encoder.bias'])\n",
    "# torch_model.node_encoder_norm.weight = nn.Parameter(\n",
    "#     state_dict['model_state_dict']['bn_node_feature.weight']\n",
    "# )\n",
    "# torch_model.node_encoder_norm.norm.weight = torch_model.node_encoder_norm.weight # this is temporary soln to the structure of the class\n",
    "\n",
    "# torch_model.node_encoder_norm.bias = nn.Parameter(\n",
    "#     state_dict['model_state_dict']['bn_node_feature.bias']\n",
    "# )\n",
    "# torch_model.node_encoder_norm.norm.bias = torch_model.node_encoder_norm.bias # this is temporary soln to the structure of the class\n",
    "\n",
    "# torch_model.node_encoder_norm.running_mean = nn.Parameter(\n",
    "#     state_dict['model_state_dict']['bn_node_feature.running_mean']\n",
    "# )\n",
    "# torch_model.node_encoder_norm.norm.running_mean = torch_model.node_encoder_norm.running_mean # this is temporary soln to the structure of the class\n",
    "\n",
    "# torch_model.node_encoder_norm.running_var = nn.Parameter(\n",
    "#     state_dict['model_state_dict']['bn_node_feature.running_var']\n",
    "# )\n",
    "# torch_model.node_encoder_norm.norm.running_var = torch_model.node_encoder_norm.running_var # this is temporary soln to the structure of the class\n",
    "\n",
    "\n",
    "# torch_model.edge_encoder_norm.weight = nn.Parameter(\n",
    "#     state_dict['model_state_dict']['bn_edge_feature.weight']\n",
    "# )\n",
    "# torch_model.edge_encoder_norm.norm.weight = torch_model.edge_encoder_norm.weight # this is temporary soln to the structure of the class\n",
    "\n",
    "# torch_model.edge_encoder_norm.bias = nn.Parameter(\n",
    "#     state_dict['model_state_dict']['bn_edge_feature.bias']\n",
    "# )\n",
    "# torch_model.edge_encoder_norm.norm.bias = torch_model.edge_encoder_norm.bias # this is temporary soln to the structure of the class\n",
    "\n",
    "# torch_model.edge_encoder_norm.running_mean = nn.Parameter(\n",
    "#     state_dict['model_state_dict']['bn_edge_feature.running_mean']\n",
    "# )\n",
    "# torch_model.edge_encoder_norm.norm.running_mean = torch_model.edge_encoder_norm.running_mean # this is temporary soln to the structure of the class\n",
    "\n",
    "# torch_model.edge_encoder_norm.running_var = nn.Parameter(\n",
    "#     state_dict['model_state_dict']['bn_edge_feature.running_var']\n",
    "# )\n",
    "# torch_model.edge_encoder_norm.norm.running_var = torch_model.edge_encoder_norm.running_var # this is temporary soln to the structure of the class\n",
    "\n",
    "\n",
    "\n",
    "torch_model.edge_encoder_norm.weight = nn.Parameter(\n",
    "    state_dict['model_state_dict']['bn_edge_feature.weight']\n",
    ")\n",
    "torch_model.edge_encoder_norm.norm.weight = torch_model.edge_encoder_norm.weight # this is temporary soln to the structure of the class\n",
    "\n",
    "torch_model.edge_encoder_norm.bias = nn.Parameter(\n",
    "    state_dict['model_state_dict']['bn_edge_feature.bias']\n",
    ")\n",
    "torch_model.edge_encoder_norm.norm.bias = torch_model.edge_encoder_norm.bias # this is temporary soln to the structure of the class\n",
    "\n",
    "torch_model.edge_encoder_norm.running_mean = nn.Parameter(\n",
    "    state_dict['model_state_dict']['bn_edge_feature.running_mean']\n",
    ")\n",
    "torch_model.edge_encoder_norm.norm.running_mean = torch_model.edge_encoder_norm.running_mean # this is temporary soln to the structure of the class\n",
    "\n",
    "torch_model.edge_encoder_norm.running_var = nn.Parameter(\n",
    "    state_dict['model_state_dict']['bn_edge_feature.running_var']\n",
    ")\n",
    "torch_model.edge_encoder_norm.norm.running_var = torch_model.edge_encoder_norm.running_var # this is temporary soln to the structure of the class\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# now the nodeblocks and betas\n",
    "original_layer_idxs = [0,1,4] # don't ask me why it jumps from 1 to 4\n",
    "new_layer_mlp_idxs = [0,1,3] # we skip 2 bc that's activation\n",
    "Betas = []\n",
    "for nodeblock_idx in range(n_layers):\n",
    "    gnn = torch_model.gnns[nodeblock_idx]\n",
    "    gnn.beta = state_dict['model_state_dict'][f'convs.{nodeblock_idx}.t']\n",
    "    Betas.append(float(gnn.beta[0]))\n",
    "    \n",
    "    mlp_name = f\"mlps.{nodeblock_idx}.\"\n",
    "    \n",
    "    for idx in range(len(original_layer_idxs)):\n",
    "        original_layer_idx = original_layer_idxs[idx]\n",
    "        new_layer_mlp_idx = new_layer_mlp_idxs[idx]\n",
    "        nodeblock_name = f\"O_{nodeblock_idx}\"\n",
    "        nodeblock = getattr(torch_model, nodeblock_name)\n",
    "        module = nodeblock.layers[new_layer_mlp_idx]\n",
    "        if (module.__class__.__name__ == 'Linear') or (module.__class__.__name__ == 'BatchNorm1d'):\n",
    "            module.weight = nn.Parameter(\n",
    "                state_dict['model_state_dict'][mlp_name+f\"{original_layer_idx}.weight\"]\n",
    "            )\n",
    "            module.bias = nn.Parameter(\n",
    "                state_dict['model_state_dict'][mlp_name+f\"{original_layer_idx}.bias\"]\n",
    "            )\n",
    "        if (module.__class__.__name__ == 'BatchNorm1d'):\n",
    "            module.running_mean = nn.Parameter(\n",
    "                state_dict['model_state_dict'][mlp_name+f\"{original_layer_idx}.running_mean\"]\n",
    "            )\n",
    "            module.running_var = nn.Parameter(\n",
    "                state_dict['model_state_dict'][mlp_name+f\"{original_layer_idx}.running_var\"]\n",
    "            )\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "54d9a428",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batchnorm_st_dict = OrderedDict()\n",
    "batchnorm_st_dict[\"weight\"] = state_dict['model_state_dict']['bn_node_feature.weight']\n",
    "batchnorm_st_dict[\"bias\"] = state_dict['model_state_dict']['bn_node_feature.bias']\n",
    "batchnorm_st_dict[\"running_mean\"] = state_dict['model_state_dict']['bn_node_feature.running_mean']\n",
    "batchnorm_st_dict[\"running_var\"] = state_dict['model_state_dict']['bn_node_feature.running_var']\n",
    "torch_model.node_encoder_norm.norm.load_state_dict(batchnorm_st_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d178043f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch_model.node_encoder_norm.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a122813a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "beta loading for layer 2 successful: tensor([True])\n",
      "weight loading for nodeblock 0 layer 0 successful: True\n",
      "bias loading for nodeblock 0 layer 0 successful: True\n",
      "weight loading for nodeblock 0 layer 1 successful: True\n",
      "bias loading for nodeblock 0 layer 1 successful: True\n",
      "running_mean loading for nodeblock 0 layer 1 successful: True\n",
      "running_var loading for nodeblock 0 layer 1 successful: True\n",
      "weight loading for nodeblock 0 layer 2 successful: True\n",
      "bias loading for nodeblock 0 layer 2 successful: True\n",
      "beta loading for layer 2 successful: tensor([True])\n",
      "weight loading for nodeblock 1 layer 0 successful: True\n",
      "bias loading for nodeblock 1 layer 0 successful: True\n",
      "weight loading for nodeblock 1 layer 1 successful: True\n",
      "bias loading for nodeblock 1 layer 1 successful: True\n",
      "running_mean loading for nodeblock 1 layer 1 successful: True\n",
      "running_var loading for nodeblock 1 layer 1 successful: True\n",
      "weight loading for nodeblock 1 layer 2 successful: True\n",
      "bias loading for nodeblock 1 layer 2 successful: True\n",
      "beta loading for layer 2 successful: tensor([True])\n",
      "weight loading for nodeblock 2 layer 0 successful: True\n",
      "bias loading for nodeblock 2 layer 0 successful: True\n",
      "weight loading for nodeblock 2 layer 1 successful: True\n",
      "bias loading for nodeblock 2 layer 1 successful: True\n",
      "running_mean loading for nodeblock 2 layer 1 successful: True\n",
      "running_var loading for nodeblock 2 layer 1 successful: True\n",
      "weight loading for nodeblock 2 layer 2 successful: True\n",
      "bias loading for nodeblock 2 layer 2 successful: True\n",
      "beta loading for layer 2 successful: tensor([True])\n",
      "weight loading for nodeblock 3 layer 0 successful: True\n",
      "bias loading for nodeblock 3 layer 0 successful: True\n",
      "weight loading for nodeblock 3 layer 1 successful: True\n",
      "bias loading for nodeblock 3 layer 1 successful: True\n",
      "running_mean loading for nodeblock 3 layer 1 successful: True\n",
      "running_var loading for nodeblock 3 layer 1 successful: True\n",
      "weight loading for nodeblock 3 layer 2 successful: True\n",
      "bias loading for nodeblock 3 layer 2 successful: True\n",
      "beta loading for layer 2 successful: tensor([True])\n",
      "weight loading for nodeblock 4 layer 0 successful: True\n",
      "bias loading for nodeblock 4 layer 0 successful: True\n",
      "weight loading for nodeblock 4 layer 1 successful: True\n",
      "bias loading for nodeblock 4 layer 1 successful: True\n",
      "running_mean loading for nodeblock 4 layer 1 successful: True\n",
      "running_var loading for nodeblock 4 layer 1 successful: True\n",
      "weight loading for nodeblock 4 layer 2 successful: True\n",
      "bias loading for nodeblock 4 layer 2 successful: True\n",
      "beta loading for layer 2 successful: tensor([True])\n",
      "weight loading for nodeblock 5 layer 0 successful: True\n",
      "bias loading for nodeblock 5 layer 0 successful: True\n",
      "weight loading for nodeblock 5 layer 1 successful: True\n",
      "bias loading for nodeblock 5 layer 1 successful: True\n",
      "running_mean loading for nodeblock 5 layer 1 successful: True\n",
      "running_var loading for nodeblock 5 layer 1 successful: True\n",
      "weight loading for nodeblock 5 layer 2 successful: True\n",
      "bias loading for nodeblock 5 layer 2 successful: True\n",
      "beta loading for layer 2 successful: tensor([True])\n",
      "weight loading for nodeblock 6 layer 0 successful: True\n",
      "bias loading for nodeblock 6 layer 0 successful: True\n",
      "weight loading for nodeblock 6 layer 1 successful: True\n",
      "bias loading for nodeblock 6 layer 1 successful: True\n",
      "running_mean loading for nodeblock 6 layer 1 successful: True\n",
      "running_var loading for nodeblock 6 layer 1 successful: True\n",
      "weight loading for nodeblock 6 layer 2 successful: True\n",
      "bias loading for nodeblock 6 layer 2 successful: True\n",
      "beta loading for layer 2 successful: tensor([True])\n",
      "weight loading for nodeblock 7 layer 0 successful: True\n",
      "bias loading for nodeblock 7 layer 0 successful: True\n",
      "weight loading for nodeblock 7 layer 1 successful: True\n",
      "bias loading for nodeblock 7 layer 1 successful: True\n",
      "running_mean loading for nodeblock 7 layer 1 successful: True\n",
      "running_var loading for nodeblock 7 layer 1 successful: True\n",
      "weight loading for nodeblock 7 layer 2 successful: True\n",
      "bias loading for nodeblock 7 layer 2 successful: True\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Just some code to test if the transfer was successful\n",
    "\"\"\"\n",
    "for nodeblock_idx in range(n_layers):\n",
    "    gnn = torch_model.gnns[nodeblock_idx]\n",
    "    boolean_val = gnn.beta == state_dict['model_state_dict'][f'convs.{nodeblock_idx}.t']\n",
    "#     print(f\"beta: {gnn.beta}\")\n",
    "    print(f\"beta loading for layer {idx} successful: {boolean_val}\")\n",
    "    \n",
    "    mlp_name = f\"mlps.{nodeblock_idx}.\"\n",
    "    for idx in range(len(original_layer_idxs)):\n",
    "        original_layer_idx = original_layer_idxs[idx]\n",
    "        new_layer_mlp_idx = new_layer_mlp_idxs[idx]\n",
    "        nodeblock_name = f\"O_{nodeblock_idx}\"\n",
    "        nodeblock = getattr(torch_model, nodeblock_name)\n",
    "        module = nodeblock.layers[new_layer_mlp_idx]\n",
    "        if (module.__class__.__name__ == 'Linear') or (module.__class__.__name__ == 'BatchNorm1d'):\n",
    "            boolean_val = torch.all(\n",
    "                module.state_dict()[\"weight\"] == state_dict['model_state_dict'][mlp_name+f\"{original_layer_idx}.weight\"]\n",
    "            )\n",
    "            print(f\"weight loading for nodeblock {nodeblock_idx} layer {idx} successful: {boolean_val}\")\n",
    "            \n",
    "            boolean_val = torch.all(\n",
    "                module.state_dict()[\"bias\"] == state_dict['model_state_dict'][mlp_name+f\"{original_layer_idx}.bias\"]\n",
    "            )\n",
    "            print(f\"bias loading for nodeblock {nodeblock_idx} layer {idx} successful: {boolean_val}\")\n",
    "            \n",
    "        if (module.__class__.__name__ == 'BatchNorm1d'):\n",
    "            boolean_val = torch.all(\n",
    "                module.state_dict()[\"running_mean\"] == state_dict['model_state_dict'][mlp_name+f\"{original_layer_idx}.running_mean\"]\n",
    "            )\n",
    "            print(f\"running_mean loading for nodeblock {nodeblock_idx} layer {idx} successful: {boolean_val}\")\n",
    "            \n",
    "            boolean_val = torch.all(\n",
    "                module.state_dict()[\"running_var\"] == state_dict['model_state_dict'][mlp_name+f\"{original_layer_idx}.running_var\"]\n",
    "            )\n",
    "            print(f\"running_var loading for nodeblock {nodeblock_idx} layer {idx} successful: {boolean_val}\")\n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "24445b9b",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'Tau3MuGNNs'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_94507/2487040273.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0msiqi\u001b[0m\u001b[0;31m'\u001b[0m\u001b[0ms\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \"\"\"\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mTau3MuGNNs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mModel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'Tau3MuGNNs'"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "siqi's model\n",
    "\"\"\"\n",
    "from Tau3MuGNNs.src.models import Model\n",
    "import torch\n",
    "\n",
    "config = {\n",
    "    \"bn_input\": True,                 # Batch normalization on input features? This is to normalize the input features\n",
    "  \"n_layers\": 8    ,                # Number of GNN layers\n",
    "  \"out_channels\": 128  ,            # Number of hidden channels for each GNN layer\n",
    "  \"dropout_p\": 0.5  ,               # Dropout probability\n",
    "  \"readout\": \"pool\"  ,                # Specify the method to use for the readout layer. One can also use 'lstm', 'vn' or 'jknet'\n",
    "  \"norm_type\": \"batch\"   ,            # Specify the type of normalization to use. One can also use 'instance', 'layer' or 'graph'\n",
    "  \"deepgcn_aggr\": \"softmax\"          # Aggregation function for the DeeperGCN layers. Please refer to their documentation for more details\n",
    "}\n",
    "x_dim = 3\n",
    "edge_attr_dim = 4\n",
    "\n",
    "model_siqi = Model(x_dim, edge_attr_dim, True, config).eval()\n",
    "\n",
    "state_dict = torch.load('./model.pt', map_location=\"cpu\")\n",
    "\n",
    "# model = torch.jit.load('./model.pt', map_location=\"cpu\")\n",
    "\n",
    "model_siqi.load_state_dict(state_dict['model_state_dict'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e49e754",
   "metadata": {},
   "source": [
    "### HLS Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2497dd99",
   "metadata": {},
   "source": [
    "hls4ml cannot infer the *order* in which these submodules are called within the pytorch model's \"forward()\" function. We have to manually define this information in the form of an ordered-dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "18b64da5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "forward_dict: defines the order in which graph-blocks are called in the model's 'forward()' method\n",
    "\"\"\"\n",
    "forward_dict = OrderedDict()\n",
    "forward_dict[\"node_encoder\"] = \"NodeEncoder\"\n",
    "forward_dict[\"edge_encoder\"] = \"EdgeEncoder\"\n",
    "forward_dict[\"node_encoder_norm\"] = \"NodeEncoderBatchNorm1d\"\n",
    "forward_dict[\"edge_encoder_norm\"] = \"EdgeEncoderBatchNorm1d\"\n",
    "for nodeblock_idx in range(n_layers):\n",
    "    forward_dict[f\"O_{nodeblock_idx}\"] = \"NodeBlock\"\n",
    "forward_dict[\"pool\"] = \"MeanPool\"  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbf43159",
   "metadata": {},
   "source": [
    "hls4ml creates a hardware implementation of the GNN, which can only be represented using fixed-size arrays. This restriction also applies to the inputs and outputs of the GNN, so we must define the size of the graphs that this hardware GNN can take as input**, again in the form of a dictionary. \n",
    "\n",
    "**Graphs of a different size can be padded or truncated to the appropriate size using the \"fix_graph_size\" function. In this notebook, padding/truncation is  done in the \"Data\" cell. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "baa5a9d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "we define additional parameters.\n",
    "\"\"\"\n",
    "common_dim = 128\n",
    "graph_dims = {\n",
    "        \"n_node\": 28,\n",
    "        \"n_edge\": 37,\n",
    "        \"node_attr\": 3,\n",
    "        \"node_dim\": common_dim,\n",
    "        \"edge_attr\": 4,\n",
    "    \"edge_dim\":common_dim\n",
    "}\n",
    "\n",
    "misc_config = {\"Betas\" : Betas}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bed5a799",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch_model.state_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c943157a",
   "metadata": {},
   "source": [
    "Armed with our pytorch model and these two dictionaries**, we can create the HLS model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "59aa1957",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "config: {'Model': {'Precision': 'ap_fixed<64,30>', 'IndexPrecision': 'ap_uint<16>', 'ReuseFactor': 8, 'Strategy': 'Latency'}}\n",
      "self.torch_model: GENConvBig(\n",
      "  (node_encoder): Linear(in_features=3, out_features=128, bias=True)\n",
      "  (node_encoder_norm): NodeEncoderBatchNorm1d(\n",
      "    (norm): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (edge_encoder): Linear(in_features=4, out_features=128, bias=True)\n",
      "  (edge_encoder_norm): EdgeEncoderBatchNorm1d(\n",
      "    (norm): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (gnns): ModuleList(\n",
      "    (0): GENConvSmall()\n",
      "    (1): GENConvSmall()\n",
      "    (2): GENConvSmall()\n",
      "    (3): GENConvSmall()\n",
      "    (4): GENConvSmall()\n",
      "    (5): GENConvSmall()\n",
      "    (6): GENConvSmall()\n",
      "    (7): GENConvSmall()\n",
      "  )\n",
      "  (O_0): ObjectModel(\n",
      "    (layers): Sequential(\n",
      "      (0): Linear(in_features=128, out_features=256, bias=True)\n",
      "      (1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): LeakyReLU(negative_slope=0.01)\n",
      "      (3): Linear(in_features=256, out_features=128, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (O_1): ObjectModel(\n",
      "    (layers): Sequential(\n",
      "      (0): Linear(in_features=128, out_features=256, bias=True)\n",
      "      (1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): LeakyReLU(negative_slope=0.01)\n",
      "      (3): Linear(in_features=256, out_features=128, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (O_2): ObjectModel(\n",
      "    (layers): Sequential(\n",
      "      (0): Linear(in_features=128, out_features=256, bias=True)\n",
      "      (1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): LeakyReLU(negative_slope=0.01)\n",
      "      (3): Linear(in_features=256, out_features=128, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (O_3): ObjectModel(\n",
      "    (layers): Sequential(\n",
      "      (0): Linear(in_features=128, out_features=256, bias=True)\n",
      "      (1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): LeakyReLU(negative_slope=0.01)\n",
      "      (3): Linear(in_features=256, out_features=128, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (O_4): ObjectModel(\n",
      "    (layers): Sequential(\n",
      "      (0): Linear(in_features=128, out_features=256, bias=True)\n",
      "      (1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): LeakyReLU(negative_slope=0.01)\n",
      "      (3): Linear(in_features=256, out_features=128, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (O_5): ObjectModel(\n",
      "    (layers): Sequential(\n",
      "      (0): Linear(in_features=128, out_features=256, bias=True)\n",
      "      (1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): LeakyReLU(negative_slope=0.01)\n",
      "      (3): Linear(in_features=256, out_features=128, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (O_6): ObjectModel(\n",
      "    (layers): Sequential(\n",
      "      (0): Linear(in_features=128, out_features=256, bias=True)\n",
      "      (1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): LeakyReLU(negative_slope=0.01)\n",
      "      (3): Linear(in_features=256, out_features=128, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (O_7): ObjectModel(\n",
      "    (layers): Sequential(\n",
      "      (0): Linear(in_features=128, out_features=256, bias=True)\n",
      "      (1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): LeakyReLU(negative_slope=0.01)\n",
      "      (3): Linear(in_features=256, out_features=128, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "misc_config: {'Betas': [3.8588860034942627, 1.0975205898284912, 1.655377745628357, 1.9278353452682495, 1.6415308713912964, 1.491963267326355, 1.4540245532989502, 1.184328317642212]}\n",
      " Mean pool initialize inp: <hls4ml.model.hls_layers.ArrayVariable object at 0x7f9361e5e150>\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "We initialize hls model from pyg model\n",
    "\"\"\"\n",
    "output_dir = \"test_GNN\"\n",
    "config = config_from_pyg_model(torch_model,\n",
    "                                   default_precision=\"ap_fixed<64,30>\",\n",
    "                                   default_index_precision='ap_uint<16>', \n",
    "                                   default_reuse_factor=8)\n",
    "print(f\"config: {config}\")\n",
    "hls_model = convert_from_pyg_model(torch_model,\n",
    "                                       n_edge=graph_dims['n_edge'],\n",
    "                                       n_node=graph_dims['n_node'],\n",
    "                                       edge_attr=graph_dims['edge_attr'],\n",
    "                                       node_attr=graph_dims['node_attr'],\n",
    "                                       edge_dim=graph_dims['edge_dim'],\n",
    "                                       node_dim=graph_dims['node_dim'],\n",
    "                                       misc_config = misc_config,\n",
    "                                       forward_dictionary=forward_dict, \n",
    "                                       activate_final='sigmoid', #sigmoid\n",
    "                                       output_dir=output_dir,\n",
    "                                       hls_config=config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8f0701c",
   "metadata": {},
   "source": [
    "## hls_model.compile() builds the C-function for the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7da705cc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing HLS project\n",
      "outputs_str: result_t layer25_out[N_LAYER_1_4*LAYER23_OUT_DIM]\n",
      "layer: <hls4ml.model.hls_layers.Input object at 0x7f93622ac7d0>\n",
      "layer: <hls4ml.model.hls_layers.Input object at 0x7f93622ac450>\n",
      "layer: <hls4ml.model.hls_layers.Input object at 0x7f93622ac8d0>\n",
      "layer: <hls4ml.model.hls_layers.NodeEncoder object at 0x7f93622ac410>\n",
      "def_cpp: layer4_t layer4_out[N_LAYER_1_4*N_LAYER_2_4]\n",
      "layer: <hls4ml.model.hls_layers.EdgeEncoder object at 0x7f9361fc6c10>\n",
      "def_cpp: layer5_t layer5_out[N_LAYER_1_5*N_LAYER_2_5]\n",
      "layer: <hls4ml.model.hls_layers.BatchNorm2D object at 0x7f9361fc6590>\n",
      "def_cpp: layer6_t layer6_out[N_LAYER_1_4*N_LAYER_2_4]\n",
      "layer: <hls4ml.model.hls_layers.BatchNorm2D object at 0x7f93622ac850>\n",
      "def_cpp: layer7_t layer7_out[N_LAYER_1_5*N_LAYER_2_5]\n",
      "layer: <hls4ml.model.hls_layers.EdgeAggregate object at 0x7f9361ff69d0>\n",
      "def_cpp: layer8_t layer8_out[N_NODE*LAYER8_OUT_DIM]\n",
      "layer: <hls4ml.model.hls_layers.NodeBlock object at 0x7f9361ffb1d0>\n",
      "def_cpp: layer9_t layer9_out[N_LAYER_1_4*LAYER9_OUT_DIM]\n",
      "layer: <hls4ml.model.hls_layers.EdgeAggregate object at 0x7f9361ffb050>\n",
      "def_cpp: layer10_t layer10_out[N_NODE*LAYER10_OUT_DIM]\n",
      "layer: <hls4ml.model.hls_layers.NodeBlock object at 0x7f9361f83c10>\n",
      "def_cpp: layer11_t layer11_out[N_LAYER_1_4*LAYER11_OUT_DIM]\n",
      "layer: <hls4ml.model.hls_layers.EdgeAggregate object at 0x7f9361f83990>\n",
      "def_cpp: layer12_t layer12_out[N_NODE*LAYER12_OUT_DIM]\n",
      "layer: <hls4ml.model.hls_layers.NodeBlock object at 0x7f9361f89ed0>\n",
      "def_cpp: layer13_t layer13_out[N_LAYER_1_4*LAYER13_OUT_DIM]\n",
      "layer: <hls4ml.model.hls_layers.EdgeAggregate object at 0x7f9361f8ef50>\n",
      "def_cpp: layer14_t layer14_out[N_NODE*LAYER14_OUT_DIM]\n",
      "layer: <hls4ml.model.hls_layers.NodeBlock object at 0x7f9361f92250>\n",
      "def_cpp: layer15_t layer15_out[N_LAYER_1_4*LAYER15_OUT_DIM]\n",
      "layer: <hls4ml.model.hls_layers.EdgeAggregate object at 0x7f9361f922d0>\n",
      "def_cpp: layer16_t layer16_out[N_NODE*LAYER16_OUT_DIM]\n",
      "layer: <hls4ml.model.hls_layers.NodeBlock object at 0x7f9361f9b090>\n",
      "def_cpp: layer17_t layer17_out[N_LAYER_1_4*LAYER17_OUT_DIM]\n",
      "layer: <hls4ml.model.hls_layers.EdgeAggregate object at 0x7f9361f9b210>\n",
      "def_cpp: layer18_t layer18_out[N_NODE*LAYER18_OUT_DIM]\n",
      "layer: <hls4ml.model.hls_layers.NodeBlock object at 0x7f9361e4da90>\n",
      "def_cpp: layer19_t layer19_out[N_LAYER_1_4*LAYER19_OUT_DIM]\n",
      "layer: <hls4ml.model.hls_layers.EdgeAggregate object at 0x7f9361e4dad0>\n",
      "def_cpp: layer20_t layer20_out[N_NODE*LAYER20_OUT_DIM]\n",
      "layer: <hls4ml.model.hls_layers.NodeBlock object at 0x7f9361e52dd0>\n",
      "def_cpp: layer21_t layer21_out[N_LAYER_1_4*LAYER21_OUT_DIM]\n",
      "layer: <hls4ml.model.hls_layers.EdgeAggregate object at 0x7f9361e52e10>\n",
      "def_cpp: layer22_t layer22_out[N_NODE*LAYER22_OUT_DIM]\n",
      "layer: <hls4ml.model.hls_layers.NodeBlock object at 0x7f9361e5a150>\n",
      "def_cpp: layer23_t layer23_out[N_LAYER_1_4*LAYER23_OUT_DIM]\n",
      "layer: <hls4ml.model.hls_layers.MeanPool object at 0x7f9361e5a190>\n",
      "def_cpp: layer24_t layer24_out[128]\n",
      "final Mean pool template: nnet::mean_pool<layer23_t, layer24_t, config24>(layer23_out, layer24_out);\n",
      "layer: <hls4ml.model.hls_layers.Activation object at 0x7f9361e5e550>\n",
      "final Mean pool template: nnet::mean_pool<layer23_t, layer24_t, config24>(layer23_out, layer24_out);\n",
      "Done\n",
      "lib_name: firmware/myproject-fA51BBD8.so\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\ncompile\\nbuild\\nimplementation\\n'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hls_model.compile()\n",
    "# hls_model.build()\n",
    "\"\"\"\n",
    "compile\n",
    "build\n",
    "implementation\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e678b04",
   "metadata": {},
   "source": [
    "# Evaluation and prediction: hls_model.predict(input)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "e31a4c5a",
   "metadata": {},
   "source": [
    "If your model takes a non-singular input (e.g. node attributes, edge attributes, and an edge index), then you should pass it as a list (e.g. [node_attr, edge_attr, edge_index]). See the \"data_wrapper\" class, and note that the hls_model.predict() method is used on the data.hls_data attribute. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "661c3eaa",
   "metadata": {},
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df4856a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class data_wrapper(object):\n",
    "    def __init__(self, node_attr, edge_attr, edge_index, target):\n",
    "        self.x = node_attr\n",
    "        self.edge_attr = edge_attr\n",
    "        self.edge_index = edge_index.transpose(0,1)\n",
    "\n",
    "        node_attr, edge_attr, edge_index = self.x.detach().cpu().numpy(), self.edge_attr.detach().cpu().numpy(), self.edge_index.transpose(0, 1).detach().cpu().numpy().astype(np.float32)\n",
    "        node_attr, edge_attr, edge_index = np.ascontiguousarray(node_attr), np.ascontiguousarray(edge_attr), np.ascontiguousarray(edge_index)\n",
    "        self.hls_data = [node_attr, edge_attr, edge_index]\n",
    "\n",
    "        self.target = target\n",
    "        self.np_target = np.reshape(target.detach().cpu().numpy(), newshape=(target.shape[0],))\n",
    "\n",
    "def load_graphs(graph_indir, graph_dims, n_graphs):\n",
    "    graph_files = np.array(os.listdir(graph_indir))\n",
    "    graph_files = np.array([os.path.join(graph_indir, graph_file)\n",
    "                            for graph_file in graph_files])\n",
    "    n_graphs_total = len(graph_files)\n",
    "    IDs = np.arange(n_graphs_total)\n",
    "    print(f\"IDS: {IDs}\")\n",
    "    dataset = GraphDataset(graph_files=graph_files[IDs])\n",
    "\n",
    "    graphs = []\n",
    "    for data in dataset[:n_graphs]:\n",
    "        node_attr, edge_attr, edge_index, target, bad_graph = fix_graph_size(data.x, data.edge_attr, data.edge_index,\n",
    "                                                                             data.y,\n",
    "                                                                             n_node_max=graph_dims['n_node'],\n",
    "                                                                             n_edge_max=graph_dims['n_edge'])\n",
    "        if not bad_graph:\n",
    "            graphs.append(data_wrapper(node_attr, edge_attr, edge_index, target))\n",
    "#         graphs.append(data_wrapper(node_attr, edge_attr, edge_index, target))\n",
    "    print(f\"graphs length: {len(graphs)}\")\n",
    "\n",
    "    print(\"writing test bench data for 1st graph\")\n",
    "    data = graphs[0]\n",
    "    node_attr, edge_attr, edge_index = data.x.detach().cpu().numpy(), data.edge_attr.detach().cpu().numpy(), data.edge_index.transpose(\n",
    "        0, 1).detach().cpu().numpy().astype(np.int32)\n",
    "    os.makedirs('tb_data', exist_ok=True)\n",
    "    input_data = np.concatenate([node_attr.reshape(1, -1), edge_attr.reshape(1, -1), edge_index.reshape(1, -1)], axis=1)\n",
    "    np.savetxt('tb_data/input_data.dat', input_data, fmt='%f', delimiter=' ')\n",
    "\n",
    "    return graphs\n",
    "\n",
    "\n",
    "graph_indir = \"trackml_data/processed_plus_pyg_small\"\n",
    "\n",
    "graphs = load_graphs(graph_indir, graph_dims, n_graphs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "351c75eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Here we are testing hls model output compared to pyg model.\n",
    "We are using Mean Squared Error (MSE) to calculate the differences \n",
    "in the output of the two models.\n",
    "\"\"\"\n",
    "MSE_l = []\n",
    "batch = None\n",
    "siqi_data = None\n",
    "for data in graphs:\n",
    "    torch_pred = torch_model(data)\n",
    "    torch_pred = torch_pred.detach().cpu().numpy().flatten()\n",
    "    hls_pred = hls_model.predict(data.hls_data)\n",
    "    siqi_pred = model_siqi(\n",
    "        x = data.x, edge_index = data.edge_index, edge_attr = data.edge_attr, batch = None, data = None\n",
    "    )\n",
    "    siqi_pred = siqi_pred.detach().cpu().numpy().flatten()\n",
    "    print(f\"torch_pred.shape: {torch_pred.shape}\")\n",
    "    print(f\"hls_pred.shape: {hls_pred.shape}\")\n",
    "    MSE = mean_squared_error(torch_pred, hls_pred)\n",
    "#     print(np.testing.assert_almost_equal(torch_pred, hls_pred))\n",
    "#     MSE = mean_squared_error(torch_pred, siqi_pred)\n",
    "    MSE_l.append(MSE)\n",
    "\n",
    "MSE_l = np.array(MSE_l)\n",
    "print(f\"MSE_l: {MSE_l}\")\n",
    "print(f\"Mean of all MSEs: {np.mean(MSE_l)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "23b1428f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSEs: \n",
      " [3.0918636e-06, 3.1896177e-06, 3.2434014e-06, 3.2127532e-06, 3.3256863e-06, 3.2834487e-06, 3.0279243e-06, 3.053681e-06, 3.1698141e-06, 3.1258835e-06, 3.434475e-06, 3.106875e-06, 3.2736061e-06, 3.27869e-06, 3.290763e-06, 3.2670318e-06, 3.1058073e-06, 3.221385e-06, 3.1638313e-06, 3.1077645e-06]\n"
     ]
    }
   ],
   "source": [
    "with open('test_data.pickle', 'rb') as f:\n",
    "    graphs= pkl.load(f) \n",
    "\n",
    "MSEs = []\n",
    "for data in graphs:\n",
    "    torch_pred = torch_model(data)\n",
    "    torch_pred = torch_pred.detach().cpu().numpy().flatten()\n",
    "    hls_pred = hls_model.predict(data.hls_data)\n",
    "    MSE = mean_squared_error(torch_pred, hls_pred)\n",
    "    MSEs.append(MSE)\n",
    "    \n",
    "print(f\"MSEs: \\n {MSEs}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62330a77",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Now let's load some of tau3mu data from our group (Prof Mia Liu).\n",
    "This is still a smaller sample of the total data, but it's good enough. \n",
    "\n",
    "NOTE: this will take some time (<5mins)\n",
    "\"\"\"\n",
    "import timeit\n",
    "\n",
    "MSEs = []\n",
    "stages = [\"train\", \"valid\", \"test\"]\n",
    "# turn off debugging here\n",
    "torch_model.SetDebugMode(False)\n",
    "\n",
    "for stage in stages:\n",
    "    with open(f'tau3mu_data/test_BIG_data_{stage}.pickle', 'rb') as f:\n",
    "        graphs= pkl.load(f) \n",
    "        \n",
    "    counter = 0\n",
    "    start = timeit.default_timer()\n",
    "    for data in graphs:\n",
    "        # use counter to just keep track of the progress. Nothing fancy\n",
    "        if counter%500 ==0 and counter != 0:\n",
    "            print(f\"counter: {counter}\")\n",
    "        counter += 1\n",
    "        torch_pred = torch_model(data)\n",
    "        torch_pred = torch_pred.detach().cpu().numpy()\n",
    "        hls_pred = hls_model.predict(data.hls_data)\n",
    "        MSE = mean_squared_error(torch_pred, hls_pred)\n",
    "        MSEs.append(MSE)\n",
    "    end = timeit.default_timer()\n",
    "    print(f\"time taken: {(end - start)/ 60} mins\")\n",
    "MSEs = np.array(MSEs)\n",
    "print(f\"MSE means: {np.mean(MSEs)}\")\n",
    "print(f\"MSE max: {np.max(MSEs)}\")\n",
    "print(f\"n_total: {MSEs.shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5741bccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Now let's graph the MSE distribution\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "n_total = MSEs.shape[0]\n",
    "mean_val = np.mean(MSEs)\n",
    "\n",
    "plt.hist(MSEs, density=True, bins=50, label=f\"Mean value: {mean_val}\\n max val outlier removed\") \n",
    "plt.ylabel('Occurrence')\n",
    "plt.xlabel('MSE');\n",
    "plt.title(f'MSE of hls vs torch prediction (n_total: {n_total})')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "plt.savefig('MSEs.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e149761",
   "metadata": {},
   "source": [
    "You can see from the graph above that the error is very small (order of magnitude -7). This will obviously get bigger once you use more realistic ap_fixed parameters, but this proves that the conversion itself is working as intended.\n",
    "\n",
    "So this is the latest progress on the pyg to hls conversion. The current model is only one layer out of eight pyg layers from the original Siqi's model. More work is on the way, but hopefully this gives you a good idea of how the conversion pipeline works. \n",
    "\n",
    "For any questions, please email me at yun@purdue.edu, or slack if you already have me on it.\n",
    "Thank you!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53405494",
   "metadata": {},
   "source": [
    "# Biography\n",
    "This walkthrough and other local files were taken from Mr Abd Elabd's code at https://github.com/abdelabd/manual_GNN_conversion <br />\n",
    "The hls4ml pyg support's starting code has been taken from Mr. Abd Elabd and Prof Javier Duarte's work: https://github.com/fastmachinelearning/hls4ml/tree/pyg_to_hls_rebase_w_dataflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d94335a2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
