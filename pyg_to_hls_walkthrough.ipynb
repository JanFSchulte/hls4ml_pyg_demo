{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a1bc2776",
   "metadata": {},
   "source": [
    "# Installation\n",
    "Please follow the instructions on README.mk file for installing the necessary packages to run this notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24b78f5f",
   "metadata": {},
   "source": [
    "This walkthrough has few instructions. It's mainly just code to help the user to understand the pytorch geometric to hls4ml pipeline. If there's any confusion, please email me at yun79@purdue.edu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1f1907e",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e745ebbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-12 15:51:58.897892: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-10-12 15:51:58.897924: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "handler args: ('NodeBlock',)\n",
      "handler args: ('EdgeAggregate',)\n",
      "handler args: ('ResidualBlock',)\n",
      "handler args: ('NodeEncoder',)\n",
      "handler args: ('EdgeEncoder',)\n",
      "handler args: ('NodeEncoderBatchNorm1d',)\n",
      "handler args: ('EdgeEncoderBatchNorm1d',)\n",
      "handler args: ('MeanPool',)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import yaml\n",
    "import argparse\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from hls4ml.utils.config import config_from_pyg_model\n",
    "from hls4ml.converters import convert_from_pyg_model\n",
    "import hls4ml\n",
    "\n",
    "from collections import OrderedDict\n",
    "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score, mean_absolute_error, mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "\n",
    "# locals\n",
    "from utils.models.interaction_network_pyg import GENConvBig\n",
    "from model_wrappers import model_wrapper\n",
    "from utils.data.dataset_pyg import GraphDataset\n",
    "from utils.data.fix_graph_size import fix_graph_size\n",
    "import time\n",
    "import pickle as pkl\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb118bac",
   "metadata": {},
   "source": [
    "### PyTorch Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "24082fea",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "We intialize our custom pytorch geometric(pyg) model\n",
    "\"\"\"\n",
    "n_layers = 8\n",
    "torch_model = GENConvBig(\n",
    "    n_layers, \n",
    "    flow = \"source_to_target\",\n",
    "    out_channels = 128,\n",
    "    debugging = True\n",
    ").eval() # eval mode for bathnorm\n",
    "\"\"\"\n",
    "We obtain the state dict(trained parameters) from Siqi Miao, PhD student of Prof Pan Li\n",
    "\"\"\"\n",
    "state_dict = torch.load('./model.pt', map_location=\"cpu\")\n",
    "# state_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "602c6952",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch_model.node_encoder_norm.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bc6ffa38",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "load/transfer the state dict into our pyg model\n",
    "\"\"\"\n",
    "\n",
    "# print(type(torch_model.node_encoder.weight))\n",
    "# print(type(siqi_model_state_dict['model_state_dict']['node_encoder.weight']))\n",
    "torch_model.node_encoder.weight = nn.Parameter(state_dict['model_state_dict']['node_encoder.weight'])\n",
    "torch_model.node_encoder.bias = nn.Parameter(state_dict['model_state_dict']['node_encoder.bias'])\n",
    "torch_model.edge_encoder.weight = nn.Parameter(state_dict['model_state_dict']['edge_encoder.weight'])\n",
    "torch_model.edge_encoder.bias = nn.Parameter(state_dict['model_state_dict']['edge_encoder.bias'])\n",
    "# torch_model.node_encoder_norm.weight = nn.Parameter(\n",
    "#     state_dict['model_state_dict']['bn_node_feature.weight']\n",
    "# )\n",
    "# torch_model.node_encoder_norm.norm.weight = torch_model.node_encoder_norm.weight # this is temporary soln to the structure of the class\n",
    "\n",
    "# torch_model.node_encoder_norm.bias = nn.Parameter(\n",
    "#     state_dict['model_state_dict']['bn_node_feature.bias']\n",
    "# )\n",
    "# torch_model.node_encoder_norm.norm.bias = torch_model.node_encoder_norm.bias # this is temporary soln to the structure of the class\n",
    "\n",
    "# torch_model.node_encoder_norm.running_mean = nn.Parameter(\n",
    "#     state_dict['model_state_dict']['bn_node_feature.running_mean']\n",
    "# )\n",
    "# torch_model.node_encoder_norm.norm.running_mean = torch_model.node_encoder_norm.running_mean # this is temporary soln to the structure of the class\n",
    "\n",
    "# torch_model.node_encoder_norm.running_var = nn.Parameter(\n",
    "#     state_dict['model_state_dict']['bn_node_feature.running_var']\n",
    "# )\n",
    "# torch_model.node_encoder_norm.norm.running_var = torch_model.node_encoder_norm.running_var # this is temporary soln to the structure of the class\n",
    "\n",
    "\n",
    "# torch_model.edge_encoder_norm.weight = nn.Parameter(\n",
    "#     state_dict['model_state_dict']['bn_edge_feature.weight']\n",
    "# )\n",
    "# torch_model.edge_encoder_norm.norm.weight = torch_model.edge_encoder_norm.weight # this is temporary soln to the structure of the class\n",
    "\n",
    "# torch_model.edge_encoder_norm.bias = nn.Parameter(\n",
    "#     state_dict['model_state_dict']['bn_edge_feature.bias']\n",
    "# )\n",
    "# torch_model.edge_encoder_norm.norm.bias = torch_model.edge_encoder_norm.bias # this is temporary soln to the structure of the class\n",
    "\n",
    "# torch_model.edge_encoder_norm.running_mean = nn.Parameter(\n",
    "#     state_dict['model_state_dict']['bn_edge_feature.running_mean']\n",
    "# )\n",
    "# torch_model.edge_encoder_norm.norm.running_mean = torch_model.edge_encoder_norm.running_mean # this is temporary soln to the structure of the class\n",
    "\n",
    "# torch_model.edge_encoder_norm.running_var = nn.Parameter(\n",
    "#     state_dict['model_state_dict']['bn_edge_feature.running_var']\n",
    "# )\n",
    "# torch_model.edge_encoder_norm.norm.running_var = torch_model.edge_encoder_norm.running_var # this is temporary soln to the structure of the class\n",
    "\n",
    "\n",
    "\n",
    "torch_model.edge_encoder_norm.weight = nn.Parameter(\n",
    "    state_dict['model_state_dict']['bn_edge_feature.weight']\n",
    ")\n",
    "torch_model.edge_encoder_norm.norm.weight = torch_model.edge_encoder_norm.weight # this is temporary soln to the structure of the class\n",
    "\n",
    "torch_model.edge_encoder_norm.bias = nn.Parameter(\n",
    "    state_dict['model_state_dict']['bn_edge_feature.bias']\n",
    ")\n",
    "torch_model.edge_encoder_norm.norm.bias = torch_model.edge_encoder_norm.bias # this is temporary soln to the structure of the class\n",
    "\n",
    "torch_model.edge_encoder_norm.running_mean = nn.Parameter(\n",
    "    state_dict['model_state_dict']['bn_edge_feature.running_mean']\n",
    ")\n",
    "torch_model.edge_encoder_norm.norm.running_mean = torch_model.edge_encoder_norm.running_mean # this is temporary soln to the structure of the class\n",
    "\n",
    "torch_model.edge_encoder_norm.running_var = nn.Parameter(\n",
    "    state_dict['model_state_dict']['bn_edge_feature.running_var']\n",
    ")\n",
    "torch_model.edge_encoder_norm.norm.running_var = torch_model.edge_encoder_norm.running_var # this is temporary soln to the structure of the class\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# now the nodeblocks and betas\n",
    "original_layer_idxs = [0,1,4] # don't ask me why it jumps from 1 to 4\n",
    "new_layer_mlp_idxs = [0,1,3] # we skip 2 bc that's activation\n",
    "Betas = []\n",
    "for nodeblock_idx in range(n_layers):\n",
    "    gnn = torch_model.gnns[nodeblock_idx]\n",
    "    gnn.beta = state_dict['model_state_dict'][f'convs.{nodeblock_idx}.t']\n",
    "    Betas.append(float(gnn.beta[0]))\n",
    "    \n",
    "    mlp_name = f\"mlps.{nodeblock_idx}.\"\n",
    "    \n",
    "    for idx in range(len(original_layer_idxs)):\n",
    "        original_layer_idx = original_layer_idxs[idx]\n",
    "        new_layer_mlp_idx = new_layer_mlp_idxs[idx]\n",
    "        nodeblock_name = f\"O_{nodeblock_idx}\"\n",
    "        nodeblock = getattr(torch_model, nodeblock_name)\n",
    "        module = nodeblock.layers[new_layer_mlp_idx]\n",
    "        if (module.__class__.__name__ == 'Linear') or (module.__class__.__name__ == 'BatchNorm1d'):\n",
    "            module.weight = nn.Parameter(\n",
    "                state_dict['model_state_dict'][mlp_name+f\"{original_layer_idx}.weight\"]\n",
    "            )\n",
    "            module.bias = nn.Parameter(\n",
    "                state_dict['model_state_dict'][mlp_name+f\"{original_layer_idx}.bias\"]\n",
    "            )\n",
    "        if (module.__class__.__name__ == 'BatchNorm1d'):\n",
    "            module.running_mean = nn.Parameter(\n",
    "                state_dict['model_state_dict'][mlp_name+f\"{original_layer_idx}.running_mean\"]\n",
    "            )\n",
    "            module.running_var = nn.Parameter(\n",
    "                state_dict['model_state_dict'][mlp_name+f\"{original_layer_idx}.running_var\"]\n",
    "            )\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "54d9a428",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batchnorm_st_dict = OrderedDict()\n",
    "batchnorm_st_dict[\"weight\"] = state_dict['model_state_dict']['bn_node_feature.weight']\n",
    "batchnorm_st_dict[\"bias\"] = state_dict['model_state_dict']['bn_node_feature.bias']\n",
    "batchnorm_st_dict[\"running_mean\"] = state_dict['model_state_dict']['bn_node_feature.running_mean']\n",
    "batchnorm_st_dict[\"running_var\"] = state_dict['model_state_dict']['bn_node_feature.running_var']\n",
    "torch_model.node_encoder_norm.norm.load_state_dict(batchnorm_st_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d178043f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch_model.node_encoder_norm.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a122813a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "beta loading for layer 2 successful: tensor([True])\n",
      "weight loading for nodeblock 0 layer 0 successful: True\n",
      "bias loading for nodeblock 0 layer 0 successful: True\n",
      "weight loading for nodeblock 0 layer 1 successful: True\n",
      "bias loading for nodeblock 0 layer 1 successful: True\n",
      "running_mean loading for nodeblock 0 layer 1 successful: True\n",
      "running_var loading for nodeblock 0 layer 1 successful: True\n",
      "weight loading for nodeblock 0 layer 2 successful: True\n",
      "bias loading for nodeblock 0 layer 2 successful: True\n",
      "beta loading for layer 2 successful: tensor([True])\n",
      "weight loading for nodeblock 1 layer 0 successful: True\n",
      "bias loading for nodeblock 1 layer 0 successful: True\n",
      "weight loading for nodeblock 1 layer 1 successful: True\n",
      "bias loading for nodeblock 1 layer 1 successful: True\n",
      "running_mean loading for nodeblock 1 layer 1 successful: True\n",
      "running_var loading for nodeblock 1 layer 1 successful: True\n",
      "weight loading for nodeblock 1 layer 2 successful: True\n",
      "bias loading for nodeblock 1 layer 2 successful: True\n",
      "beta loading for layer 2 successful: tensor([True])\n",
      "weight loading for nodeblock 2 layer 0 successful: True\n",
      "bias loading for nodeblock 2 layer 0 successful: True\n",
      "weight loading for nodeblock 2 layer 1 successful: True\n",
      "bias loading for nodeblock 2 layer 1 successful: True\n",
      "running_mean loading for nodeblock 2 layer 1 successful: True\n",
      "running_var loading for nodeblock 2 layer 1 successful: True\n",
      "weight loading for nodeblock 2 layer 2 successful: True\n",
      "bias loading for nodeblock 2 layer 2 successful: True\n",
      "beta loading for layer 2 successful: tensor([True])\n",
      "weight loading for nodeblock 3 layer 0 successful: True\n",
      "bias loading for nodeblock 3 layer 0 successful: True\n",
      "weight loading for nodeblock 3 layer 1 successful: True\n",
      "bias loading for nodeblock 3 layer 1 successful: True\n",
      "running_mean loading for nodeblock 3 layer 1 successful: True\n",
      "running_var loading for nodeblock 3 layer 1 successful: True\n",
      "weight loading for nodeblock 3 layer 2 successful: True\n",
      "bias loading for nodeblock 3 layer 2 successful: True\n",
      "beta loading for layer 2 successful: tensor([True])\n",
      "weight loading for nodeblock 4 layer 0 successful: True\n",
      "bias loading for nodeblock 4 layer 0 successful: True\n",
      "weight loading for nodeblock 4 layer 1 successful: True\n",
      "bias loading for nodeblock 4 layer 1 successful: True\n",
      "running_mean loading for nodeblock 4 layer 1 successful: True\n",
      "running_var loading for nodeblock 4 layer 1 successful: True\n",
      "weight loading for nodeblock 4 layer 2 successful: True\n",
      "bias loading for nodeblock 4 layer 2 successful: True\n",
      "beta loading for layer 2 successful: tensor([True])\n",
      "weight loading for nodeblock 5 layer 0 successful: True\n",
      "bias loading for nodeblock 5 layer 0 successful: True\n",
      "weight loading for nodeblock 5 layer 1 successful: True\n",
      "bias loading for nodeblock 5 layer 1 successful: True\n",
      "running_mean loading for nodeblock 5 layer 1 successful: True\n",
      "running_var loading for nodeblock 5 layer 1 successful: True\n",
      "weight loading for nodeblock 5 layer 2 successful: True\n",
      "bias loading for nodeblock 5 layer 2 successful: True\n",
      "beta loading for layer 2 successful: tensor([True])\n",
      "weight loading for nodeblock 6 layer 0 successful: True\n",
      "bias loading for nodeblock 6 layer 0 successful: True\n",
      "weight loading for nodeblock 6 layer 1 successful: True\n",
      "bias loading for nodeblock 6 layer 1 successful: True\n",
      "running_mean loading for nodeblock 6 layer 1 successful: True\n",
      "running_var loading for nodeblock 6 layer 1 successful: True\n",
      "weight loading for nodeblock 6 layer 2 successful: True\n",
      "bias loading for nodeblock 6 layer 2 successful: True\n",
      "beta loading for layer 2 successful: tensor([True])\n",
      "weight loading for nodeblock 7 layer 0 successful: True\n",
      "bias loading for nodeblock 7 layer 0 successful: True\n",
      "weight loading for nodeblock 7 layer 1 successful: True\n",
      "bias loading for nodeblock 7 layer 1 successful: True\n",
      "running_mean loading for nodeblock 7 layer 1 successful: True\n",
      "running_var loading for nodeblock 7 layer 1 successful: True\n",
      "weight loading for nodeblock 7 layer 2 successful: True\n",
      "bias loading for nodeblock 7 layer 2 successful: True\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Just some code to test if the transfer was successful\n",
    "\"\"\"\n",
    "for nodeblock_idx in range(n_layers):\n",
    "    gnn = torch_model.gnns[nodeblock_idx]\n",
    "    boolean_val = gnn.beta == state_dict['model_state_dict'][f'convs.{nodeblock_idx}.t']\n",
    "#     print(f\"beta: {gnn.beta}\")\n",
    "    print(f\"beta loading for layer {idx} successful: {boolean_val}\")\n",
    "    \n",
    "    mlp_name = f\"mlps.{nodeblock_idx}.\"\n",
    "    for idx in range(len(original_layer_idxs)):\n",
    "        original_layer_idx = original_layer_idxs[idx]\n",
    "        new_layer_mlp_idx = new_layer_mlp_idxs[idx]\n",
    "        nodeblock_name = f\"O_{nodeblock_idx}\"\n",
    "        nodeblock = getattr(torch_model, nodeblock_name)\n",
    "        module = nodeblock.layers[new_layer_mlp_idx]\n",
    "        if (module.__class__.__name__ == 'Linear') or (module.__class__.__name__ == 'BatchNorm1d'):\n",
    "            boolean_val = torch.all(\n",
    "                module.state_dict()[\"weight\"] == state_dict['model_state_dict'][mlp_name+f\"{original_layer_idx}.weight\"]\n",
    "            )\n",
    "            print(f\"weight loading for nodeblock {nodeblock_idx} layer {idx} successful: {boolean_val}\")\n",
    "            \n",
    "            boolean_val = torch.all(\n",
    "                module.state_dict()[\"bias\"] == state_dict['model_state_dict'][mlp_name+f\"{original_layer_idx}.bias\"]\n",
    "            )\n",
    "            print(f\"bias loading for nodeblock {nodeblock_idx} layer {idx} successful: {boolean_val}\")\n",
    "            \n",
    "        if (module.__class__.__name__ == 'BatchNorm1d'):\n",
    "            boolean_val = torch.all(\n",
    "                module.state_dict()[\"running_mean\"] == state_dict['model_state_dict'][mlp_name+f\"{original_layer_idx}.running_mean\"]\n",
    "            )\n",
    "            print(f\"running_mean loading for nodeblock {nodeblock_idx} layer {idx} successful: {boolean_val}\")\n",
    "            \n",
    "            boolean_val = torch.all(\n",
    "                module.state_dict()[\"running_var\"] == state_dict['model_state_dict'][mlp_name+f\"{original_layer_idx}.running_var\"]\n",
    "            )\n",
    "            print(f\"running_var loading for nodeblock {nodeblock_idx} layer {idx} successful: {boolean_val}\")\n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "24445b9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testing\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "siqi's model\n",
    "\"\"\"\n",
    "from Tau3MuGNNs.src.models import Model\n",
    "import torch\n",
    "\n",
    "config = {\n",
    "    \"bn_input\": True,                 # Batch normalization on input features? This is to normalize the input features\n",
    "  \"n_layers\": 8    ,                # Number of GNN layers\n",
    "  \"out_channels\": 128  ,            # Number of hidden channels for each GNN layer\n",
    "  \"dropout_p\": 0.5  ,               # Dropout probability\n",
    "  \"readout\": \"pool\"  ,                # Specify the method to use for the readout layer. One can also use 'lstm', 'vn' or 'jknet'\n",
    "  \"norm_type\": \"batch\"   ,            # Specify the type of normalization to use. One can also use 'instance', 'layer' or 'graph'\n",
    "  \"deepgcn_aggr\": \"softmax\"          # Aggregation function for the DeeperGCN layers. Please refer to their documentation for more details\n",
    "}\n",
    "x_dim = 3\n",
    "edge_attr_dim = 4\n",
    "\n",
    "model_siqi = Model(x_dim, edge_attr_dim, True, config).eval()\n",
    "\n",
    "state_dict = torch.load('./model.pt', map_location=\"cpu\")\n",
    "\n",
    "# model = torch.jit.load('./model.pt', map_location=\"cpu\")\n",
    "\n",
    "model_siqi.load_state_dict(state_dict['model_state_dict'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e49e754",
   "metadata": {},
   "source": [
    "### HLS Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2497dd99",
   "metadata": {},
   "source": [
    "hls4ml cannot infer the *order* in which these submodules are called within the pytorch model's \"forward()\" function. We have to manually define this information in the form of an ordered-dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "18b64da5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "forward_dict: defines the order in which graph-blocks are called in the model's 'forward()' method\n",
    "\"\"\"\n",
    "forward_dict = OrderedDict()\n",
    "forward_dict[\"node_encoder\"] = \"NodeEncoder\"\n",
    "forward_dict[\"edge_encoder\"] = \"EdgeEncoder\"\n",
    "forward_dict[\"node_encoder_norm\"] = \"NodeEncoderBatchNorm1d\"\n",
    "forward_dict[\"edge_encoder_norm\"] = \"EdgeEncoderBatchNorm1d\"\n",
    "for nodeblock_idx in range(n_layers):\n",
    "    forward_dict[f\"O_{nodeblock_idx}\"] = \"NodeBlock\"\n",
    "forward_dict[\"pool\"] = \"MeanPool\"  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbf43159",
   "metadata": {},
   "source": [
    "hls4ml creates a hardware implementation of the GNN, which can only be represented using fixed-size arrays. This restriction also applies to the inputs and outputs of the GNN, so we must define the size of the graphs that this hardware GNN can take as input**, again in the form of a dictionary. \n",
    "\n",
    "**Graphs of a different size can be padded or truncated to the appropriate size using the \"fix_graph_size\" function. In this notebook, padding/truncation is  done in the \"Data\" cell. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "baa5a9d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "we define additional parameters.\n",
    "\"\"\"\n",
    "common_dim = 128\n",
    "graph_dims = {\n",
    "        \"n_node\": 28,\n",
    "        \"n_edge\": 37,\n",
    "        \"node_attr\": 3,\n",
    "        \"node_dim\": common_dim,\n",
    "        \"edge_attr\": 4,\n",
    "    \"edge_dim\":common_dim\n",
    "}\n",
    "\n",
    "misc_config = {\"Betas\" : Betas}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bed5a799",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch_model.state_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c943157a",
   "metadata": {},
   "source": [
    "Armed with our pytorch model and these two dictionaries**, we can create the HLS model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "59aa1957",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "config: {'Model': {'Precision': 'ap_fixed<64,30>', 'IndexPrecision': 'ap_uint<16>', 'ReuseFactor': 8, 'Strategy': 'Latency'}}\n",
      "self.torch_model: GENConvBig(\n",
      "  (node_encoder): Linear(in_features=3, out_features=128, bias=True)\n",
      "  (node_encoder_norm): NodeEncoderBatchNorm1d(\n",
      "    (norm): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (edge_encoder): Linear(in_features=4, out_features=128, bias=True)\n",
      "  (edge_encoder_norm): EdgeEncoderBatchNorm1d(\n",
      "    (norm): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (gnns): ModuleList(\n",
      "    (0): GENConvSmall()\n",
      "    (1): GENConvSmall()\n",
      "    (2): GENConvSmall()\n",
      "    (3): GENConvSmall()\n",
      "    (4): GENConvSmall()\n",
      "    (5): GENConvSmall()\n",
      "    (6): GENConvSmall()\n",
      "    (7): GENConvSmall()\n",
      "  )\n",
      "  (O_0): ObjectModel(\n",
      "    (layers): Sequential(\n",
      "      (0): Linear(in_features=128, out_features=256, bias=True)\n",
      "      (1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): LeakyReLU(negative_slope=0.01)\n",
      "      (3): Linear(in_features=256, out_features=128, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (O_1): ObjectModel(\n",
      "    (layers): Sequential(\n",
      "      (0): Linear(in_features=128, out_features=256, bias=True)\n",
      "      (1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): LeakyReLU(negative_slope=0.01)\n",
      "      (3): Linear(in_features=256, out_features=128, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (O_2): ObjectModel(\n",
      "    (layers): Sequential(\n",
      "      (0): Linear(in_features=128, out_features=256, bias=True)\n",
      "      (1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): LeakyReLU(negative_slope=0.01)\n",
      "      (3): Linear(in_features=256, out_features=128, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (O_3): ObjectModel(\n",
      "    (layers): Sequential(\n",
      "      (0): Linear(in_features=128, out_features=256, bias=True)\n",
      "      (1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): LeakyReLU(negative_slope=0.01)\n",
      "      (3): Linear(in_features=256, out_features=128, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (O_4): ObjectModel(\n",
      "    (layers): Sequential(\n",
      "      (0): Linear(in_features=128, out_features=256, bias=True)\n",
      "      (1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): LeakyReLU(negative_slope=0.01)\n",
      "      (3): Linear(in_features=256, out_features=128, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (O_5): ObjectModel(\n",
      "    (layers): Sequential(\n",
      "      (0): Linear(in_features=128, out_features=256, bias=True)\n",
      "      (1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): LeakyReLU(negative_slope=0.01)\n",
      "      (3): Linear(in_features=256, out_features=128, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (O_6): ObjectModel(\n",
      "    (layers): Sequential(\n",
      "      (0): Linear(in_features=128, out_features=256, bias=True)\n",
      "      (1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): LeakyReLU(negative_slope=0.01)\n",
      "      (3): Linear(in_features=256, out_features=128, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (O_7): ObjectModel(\n",
      "    (layers): Sequential(\n",
      "      (0): Linear(in_features=128, out_features=256, bias=True)\n",
      "      (1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): LeakyReLU(negative_slope=0.01)\n",
      "      (3): Linear(in_features=256, out_features=128, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "misc_config: {'Betas': [3.8588860034942627, 1.0975205898284912, 1.655377745628357, 1.9278353452682495, 1.6415308713912964, 1.491963267326355, 1.4540245532989502, 1.184328317642212]}\n",
      " Mean pool initialize inp: <hls4ml.model.hls_layers.ArrayVariable object at 0x7fea042aa5d0>\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "We initialize hls model from pyg model\n",
    "\"\"\"\n",
    "output_dir = \"test_GNN\"\n",
    "config = config_from_pyg_model(torch_model,\n",
    "                                   default_precision=\"ap_fixed<64,30>\",\n",
    "                                   default_index_precision='ap_uint<16>', \n",
    "                                   default_reuse_factor=8)\n",
    "print(f\"config: {config}\")\n",
    "hls_model = convert_from_pyg_model(torch_model,\n",
    "                                       n_edge=graph_dims['n_edge'],\n",
    "                                       n_node=graph_dims['n_node'],\n",
    "                                       edge_attr=graph_dims['edge_attr'],\n",
    "                                       node_attr=graph_dims['node_attr'],\n",
    "                                       edge_dim=graph_dims['edge_dim'],\n",
    "                                       node_dim=graph_dims['node_dim'],\n",
    "                                       misc_config = misc_config,\n",
    "                                       forward_dictionary=forward_dict, \n",
    "                                       activate_final='sigmoid', #sigmoid\n",
    "                                       output_dir=output_dir,\n",
    "                                       hls_config=config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8f0701c",
   "metadata": {},
   "source": [
    "## hls_model.compile() builds the C-function for the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7da705cc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing HLS project\n",
      "model_outputs[0].dim_names: ['LAYER23_OUT_DIM']\n",
      "outputs_str: result_t layer25_out[LAYER23_OUT_DIM]\n",
      "layer: <hls4ml.model.hls_layers.Input object at 0x7fea043a4d90>\n",
      "layer: <hls4ml.model.hls_layers.Input object at 0x7fea043c0450>\n",
      "layer: <hls4ml.model.hls_layers.Input object at 0x7fea043c0d90>\n",
      "layer: <hls4ml.model.hls_layers.NodeEncoder object at 0x7fea04bffe50>\n",
      "def_cpp: layer4_t layer4_out[N_LAYER_1_4*N_LAYER_2_4]\n",
      "layer: <hls4ml.model.hls_layers.EdgeEncoder object at 0x7fea043adad0>\n",
      "def_cpp: layer5_t layer5_out[N_LAYER_1_5*N_LAYER_2_5]\n",
      "layer: <hls4ml.model.hls_layers.BatchNorm2D object at 0x7fea043b3450>\n",
      "def_cpp: layer6_t layer6_out[N_LAYER_1_4*N_LAYER_2_4]\n",
      "layer: <hls4ml.model.hls_layers.BatchNorm2D object at 0x7fea043c0350>\n",
      "def_cpp: layer7_t layer7_out[N_LAYER_1_5*N_LAYER_2_5]\n",
      "layer: <hls4ml.model.hls_layers.EdgeAggregate object at 0x7fea04365d50>\n",
      "def_cpp: layer8_t layer8_out[N_NODE*LAYER8_OUT_DIM]\n",
      "layer: <hls4ml.model.hls_layers.NodeBlock object at 0x7fea04415790>\n",
      "def_cpp: layer9_t layer9_out[N_LAYER_1_4*LAYER9_OUT_DIM]\n",
      "layer: <hls4ml.model.hls_layers.EdgeAggregate object at 0x7fea04415350>\n",
      "def_cpp: layer10_t layer10_out[N_NODE*LAYER10_OUT_DIM]\n",
      "layer: <hls4ml.model.hls_layers.NodeBlock object at 0x7fea042e4d50>\n",
      "def_cpp: layer11_t layer11_out[N_LAYER_1_4*LAYER11_OUT_DIM]\n",
      "layer: <hls4ml.model.hls_layers.EdgeAggregate object at 0x7fea042e4c10>\n",
      "def_cpp: layer12_t layer12_out[N_NODE*LAYER12_OUT_DIM]\n",
      "layer: <hls4ml.model.hls_layers.NodeBlock object at 0x7fea042eeb10>\n",
      "def_cpp: layer13_t layer13_out[N_LAYER_1_4*LAYER13_OUT_DIM]\n",
      "layer: <hls4ml.model.hls_layers.EdgeAggregate object at 0x7fea042eeb90>\n",
      "def_cpp: layer14_t layer14_out[N_NODE*LAYER14_OUT_DIM]\n",
      "layer: <hls4ml.model.hls_layers.NodeBlock object at 0x7fea04303850>\n",
      "def_cpp: layer15_t layer15_out[N_LAYER_1_4*LAYER15_OUT_DIM]\n",
      "layer: <hls4ml.model.hls_layers.EdgeAggregate object at 0x7fea043038d0>\n",
      "def_cpp: layer16_t layer16_out[N_NODE*LAYER16_OUT_DIM]\n",
      "layer: <hls4ml.model.hls_layers.NodeBlock object at 0x7fea0430c150>\n",
      "def_cpp: layer17_t layer17_out[N_LAYER_1_4*LAYER17_OUT_DIM]\n",
      "layer: <hls4ml.model.hls_layers.EdgeAggregate object at 0x7fea0430c310>\n",
      "def_cpp: layer18_t layer18_out[N_NODE*LAYER18_OUT_DIM]\n",
      "layer: <hls4ml.model.hls_layers.NodeBlock object at 0x7fea042a0090>\n",
      "def_cpp: layer19_t layer19_out[N_LAYER_1_4*LAYER19_OUT_DIM]\n",
      "layer: <hls4ml.model.hls_layers.EdgeAggregate object at 0x7fea042a00d0>\n",
      "def_cpp: layer20_t layer20_out[N_NODE*LAYER20_OUT_DIM]\n",
      "layer: <hls4ml.model.hls_layers.NodeBlock object at 0x7fea042a33d0>\n",
      "def_cpp: layer21_t layer21_out[N_LAYER_1_4*LAYER21_OUT_DIM]\n",
      "layer: <hls4ml.model.hls_layers.EdgeAggregate object at 0x7fea042a3410>\n",
      "def_cpp: layer22_t layer22_out[N_NODE*LAYER22_OUT_DIM]\n",
      "layer: <hls4ml.model.hls_layers.NodeBlock object at 0x7fea042a7710>\n",
      "def_cpp: layer23_t layer23_out[LAYER23_OUT_DIM]\n",
      "layer: <hls4ml.model.hls_layers.MeanPool object at 0x7fea042a7750>\n",
      "def_cpp: layer24_t layer24_out[128]\n",
      "final Mean pool template: nnet::mean_pool<layer23_t, layer24_t, config24>(layer23_out, layer24_out);\n",
      "layer: <hls4ml.model.hls_layers.Activation object at 0x7fea042aaa90>\n",
      "final Mean pool template: nnet::mean_pool<layer23_t, layer24_t, config24>(layer23_out, layer24_out);\n",
      "Done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "In file included from firmware/parameters.h:17,\n",
      "                 from firmware/myproject.cpp:22:\n",
      "firmware/nnet_utils/nnet_graph.h: In instantiation of ‘void nnet::nodeblock_residual(data_T*, data_T*, res_T*, typename CONFIG_T::dense_config1::weight_t*, typename CONFIG_T::dense_config1::bias_t*, typename CONFIG_T::dense_config2::weight_t*, typename CONFIG_T::dense_config2::bias_t*, typename CONFIG_T::dense_config3::weight_t*, typename CONFIG_T::dense_config3::bias_t*, typename CONFIG_T::dense_config4::weight_t*, typename CONFIG_T::dense_config4::bias_t*, typename CONFIG_T::norm_config1::scale_t*, typename CONFIG_T::norm_config1::bias_t*) [with data_T = ap_fixed<64, 30>; res_T = ap_fixed<64, 30>; CONFIG_T = config23; typename CONFIG_T::dense_config1::weight_t = ap_fixed<64, 30>; typename CONFIG_T::dense_config1::bias_t = ap_fixed<64, 30>; typename CONFIG_T::dense_config2::weight_t = ap_fixed<64, 30>; typename CONFIG_T::dense_config2::bias_t = ap_fixed<64, 30>; typename CONFIG_T::dense_config3::weight_t = ap_fixed<64, 30>; typename CONFIG_T::dense_config3::bias_t = ap_fixed<64, 30>; typename CONFIG_T::dense_config4::weight_t = ap_fixed<64, 30>; typename CONFIG_T::dense_config4::bias_t = ap_fixed<64, 30>; typename CONFIG_T::norm_config1::scale_t = ap_fixed<64, 30>; typename CONFIG_T::norm_config1::bias_t = ap_fixed<64, 30>]’:\n",
      "firmware/nnet_utils/nnet_graph.h:1233:46:   required from ‘void nnet::nodeblock(data_T*, data_T*, res_T*, typename CONFIG_T::dense_config1::weight_t*, typename CONFIG_T::dense_config1::bias_t*, typename CONFIG_T::dense_config2::weight_t*, typename CONFIG_T::dense_config2::bias_t*, typename CONFIG_T::dense_config3::weight_t*, typename CONFIG_T::dense_config3::bias_t*, typename CONFIG_T::dense_config4::weight_t*, typename CONFIG_T::dense_config4::bias_t*, typename CONFIG_T::norm_config1::scale_t*, typename CONFIG_T::norm_config1::bias_t*) [with data_T = ap_fixed<64, 30>; res_T = ap_fixed<64, 30>; CONFIG_T = config23; typename CONFIG_T::dense_config1::weight_t = ap_fixed<64, 30>; typename CONFIG_T::dense_config1::bias_t = ap_fixed<64, 30>; typename CONFIG_T::dense_config2::weight_t = ap_fixed<64, 30>; typename CONFIG_T::dense_config2::bias_t = ap_fixed<64, 30>; typename CONFIG_T::dense_config3::weight_t = ap_fixed<64, 30>; typename CONFIG_T::dense_config3::bias_t = ap_fixed<64, 30>; typename CONFIG_T::dense_config4::weight_t = ap_fixed<64, 30>; typename CONFIG_T::dense_config4::bias_t = ap_fixed<64, 30>; typename CONFIG_T::norm_config1::scale_t = ap_fixed<64, 30>; typename CONFIG_T::norm_config1::bias_t = ap_fixed<64, 30>]’\n",
      "firmware/myproject.cpp:224:178:   required from here\n",
      "firmware/nnet_utils/nnet_graph.h:1211:75: error: cannot convert ‘ap_fixed<64, 30> (*)[128]’ to ‘ap_fixed<64, 30> (*)[28]’\n",
      " 1211 |     nnet::mat_to_vec<res_T, res_T, typename CONFIG_T::node_update_config>(node_update_update, node_update_1D);\n",
      "      |                                                                           ^~~~~~~~~~~~~~~~~~\n",
      "      |                                                                           |\n",
      "      |                                                                           ap_fixed<64, 30> (*)[128]\n",
      "In file included from firmware/parameters.h:11,\n",
      "                 from firmware/myproject.cpp:22:\n",
      "firmware/nnet_utils/nnet_array.h:79:12: note:   initializing argument 1 of ‘void nnet::mat_to_vec(data_T (*)[CONFIG_T::n_cols], res_T*) [with data_T = ap_fixed<64, 30>; res_T = ap_fixed<64, 30>; CONFIG_T = config23::node_update_config]’\n",
      "   79 |     data_T mat[CONFIG_T::n_rows][CONFIG_T::n_cols],\n",
      "      |     ~~~~~~~^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "g++: error: myproject.o: No such file or directory\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lib_name: firmware/myproject-9ee69765.so\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "firmware/myproject-9ee69765.so: cannot open shared object file: No such file or directory",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_35466/14049003.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mhls_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;31m# hls_model.build()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \"\"\"\n\u001b[1;32m      4\u001b[0m \u001b[0mcompile\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mbuild\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/projects/hls4ml_pyg_demo/hls4ml/model/hls_model.py\u001b[0m in \u001b[0;36mcompile\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    549\u001b[0m                 \u001b[0mdlclose_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_top_function_lib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    550\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"lib_name: {lib_name}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 551\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_top_function_lib\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mctypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcdll\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLoadLibrary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlib_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    552\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    553\u001b[0m             \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcurr_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pyg_to_hls_walkthrough/lib/python3.7/ctypes/__init__.py\u001b[0m in \u001b[0;36mLoadLibrary\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    440\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    441\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mLoadLibrary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 442\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dlltype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    443\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    444\u001b[0m \u001b[0mcdll\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLibraryLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCDLL\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pyg_to_hls_walkthrough/lib/python3.7/ctypes/__init__.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, mode, handle, use_errno, use_last_error)\u001b[0m\n\u001b[1;32m    362\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    363\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 364\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_dlopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    365\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    366\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: firmware/myproject-9ee69765.so: cannot open shared object file: No such file or directory"
     ]
    }
   ],
   "source": [
    "hls_model.compile()\n",
    "# hls_model.build()\n",
    "\"\"\"\n",
    "compile\n",
    "build\n",
    "implementation\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e678b04",
   "metadata": {},
   "source": [
    "# Evaluation and prediction: hls_model.predict(input)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "e31a4c5a",
   "metadata": {},
   "source": [
    "If your model takes a non-singular input (e.g. node attributes, edge attributes, and an edge index), then you should pass it as a list (e.g. [node_attr, edge_attr, edge_index]). See the \"data_wrapper\" class, and note that the hls_model.predict() method is used on the data.hls_data attribute. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "661c3eaa",
   "metadata": {},
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df4856a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class data_wrapper(object):\n",
    "    def __init__(self, node_attr, edge_attr, edge_index, target):\n",
    "        self.x = node_attr\n",
    "        self.edge_attr = edge_attr\n",
    "        self.edge_index = edge_index.transpose(0,1)\n",
    "\n",
    "        node_attr, edge_attr, edge_index = self.x.detach().cpu().numpy(), self.edge_attr.detach().cpu().numpy(), self.edge_index.transpose(0, 1).detach().cpu().numpy().astype(np.float32)\n",
    "        node_attr, edge_attr, edge_index = np.ascontiguousarray(node_attr), np.ascontiguousarray(edge_attr), np.ascontiguousarray(edge_index)\n",
    "        self.hls_data = [node_attr, edge_attr, edge_index]\n",
    "\n",
    "        self.target = target\n",
    "        self.np_target = np.reshape(target.detach().cpu().numpy(), newshape=(target.shape[0],))\n",
    "\n",
    "def load_graphs(graph_indir, graph_dims, n_graphs):\n",
    "    graph_files = np.array(os.listdir(graph_indir))\n",
    "    graph_files = np.array([os.path.join(graph_indir, graph_file)\n",
    "                            for graph_file in graph_files])\n",
    "    n_graphs_total = len(graph_files)\n",
    "    IDs = np.arange(n_graphs_total)\n",
    "    print(f\"IDS: {IDs}\")\n",
    "    dataset = GraphDataset(graph_files=graph_files[IDs])\n",
    "\n",
    "    graphs = []\n",
    "    for data in dataset[:n_graphs]:\n",
    "        node_attr, edge_attr, edge_index, target, bad_graph = fix_graph_size(data.x, data.edge_attr, data.edge_index,\n",
    "                                                                             data.y,\n",
    "                                                                             n_node_max=graph_dims['n_node'],\n",
    "                                                                             n_edge_max=graph_dims['n_edge'])\n",
    "        if not bad_graph:\n",
    "            graphs.append(data_wrapper(node_attr, edge_attr, edge_index, target))\n",
    "#         graphs.append(data_wrapper(node_attr, edge_attr, edge_index, target))\n",
    "    print(f\"graphs length: {len(graphs)}\")\n",
    "\n",
    "    print(\"writing test bench data for 1st graph\")\n",
    "    data = graphs[0]\n",
    "    node_attr, edge_attr, edge_index = data.x.detach().cpu().numpy(), data.edge_attr.detach().cpu().numpy(), data.edge_index.transpose(\n",
    "        0, 1).detach().cpu().numpy().astype(np.int32)\n",
    "    os.makedirs('tb_data', exist_ok=True)\n",
    "    input_data = np.concatenate([node_attr.reshape(1, -1), edge_attr.reshape(1, -1), edge_index.reshape(1, -1)], axis=1)\n",
    "    np.savetxt('tb_data/input_data.dat', input_data, fmt='%f', delimiter=' ')\n",
    "\n",
    "    return graphs\n",
    "\n",
    "\n",
    "graph_indir = \"trackml_data/processed_plus_pyg_small\"\n",
    "\n",
    "graphs = load_graphs(graph_indir, graph_dims, n_graphs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "351c75eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Here we are testing hls model output compared to pyg model.\n",
    "We are using Mean Squared Error (MSE) to calculate the differences \n",
    "in the output of the two models.\n",
    "\"\"\"\n",
    "MSE_l = []\n",
    "batch = None\n",
    "siqi_data = None\n",
    "for data in graphs:\n",
    "    torch_pred = torch_model(data)\n",
    "    torch_pred = torch_pred.detach().cpu().numpy().flatten()\n",
    "    hls_pred = hls_model.predict(data.hls_data)\n",
    "    siqi_pred = model_siqi(\n",
    "        x = data.x, edge_index = data.edge_index, edge_attr = data.edge_attr, batch = None, data = None\n",
    "    )\n",
    "    siqi_pred = siqi_pred.detach().cpu().numpy().flatten()\n",
    "    print(f\"torch_pred.shape: {torch_pred.shape}\")\n",
    "    print(f\"hls_pred.shape: {hls_pred.shape}\")\n",
    "    MSE = mean_squared_error(torch_pred, hls_pred)\n",
    "#     print(np.testing.assert_almost_equal(torch_pred, hls_pred))\n",
    "#     MSE = mean_squared_error(torch_pred, siqi_pred)\n",
    "    MSE_l.append(MSE)\n",
    "\n",
    "MSE_l = np.array(MSE_l)\n",
    "print(f\"MSE_l: {MSE_l}\")\n",
    "print(f\"Mean of all MSEs: {np.mean(MSE_l)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "23b1428f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSEs: \n",
      " [3.0918636e-06, 3.1896177e-06, 3.2434014e-06, 3.2127532e-06, 3.3256863e-06, 3.2834487e-06, 3.0279243e-06, 3.053681e-06, 3.1698141e-06, 3.1258835e-06, 3.434475e-06, 3.106875e-06, 3.2736061e-06, 3.27869e-06, 3.290763e-06, 3.2670318e-06, 3.1058073e-06, 3.221385e-06, 3.1638313e-06, 3.1077645e-06]\n"
     ]
    }
   ],
   "source": [
    "with open('test_data.pickle', 'rb') as f:\n",
    "    graphs= pkl.load(f) \n",
    "\n",
    "MSEs = []\n",
    "for data in graphs:\n",
    "    torch_pred = torch_model(data)\n",
    "    torch_pred = torch_pred.detach().cpu().numpy().flatten()\n",
    "    hls_pred = hls_model.predict(data.hls_data)\n",
    "    MSE = mean_squared_error(torch_pred, hls_pred)\n",
    "    MSEs.append(MSE)\n",
    "    \n",
    "print(f\"MSEs: \\n {MSEs}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62330a77",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Now let's load some of tau3mu data from our group (Prof Mia Liu).\n",
    "This is still a smaller sample of the total data, but it's good enough. \n",
    "\n",
    "NOTE: this will take some time (<5mins)\n",
    "\"\"\"\n",
    "import timeit\n",
    "\n",
    "MSEs = []\n",
    "stages = [\"train\", \"valid\", \"test\"]\n",
    "# turn off debugging here\n",
    "torch_model.SetDebugMode(False)\n",
    "\n",
    "for stage in stages:\n",
    "    with open(f'tau3mu_data/test_BIG_data_{stage}.pickle', 'rb') as f:\n",
    "        graphs= pkl.load(f) \n",
    "        \n",
    "    counter = 0\n",
    "    start = timeit.default_timer()\n",
    "    for data in graphs:\n",
    "        # use counter to just keep track of the progress. Nothing fancy\n",
    "        if counter%500 ==0 and counter != 0:\n",
    "            print(f\"counter: {counter}\")\n",
    "        counter += 1\n",
    "        torch_pred = torch_model(data)\n",
    "        torch_pred = torch_pred.detach().cpu().numpy()\n",
    "        hls_pred = hls_model.predict(data.hls_data)\n",
    "        MSE = mean_squared_error(torch_pred, hls_pred)\n",
    "        MSEs.append(MSE)\n",
    "    end = timeit.default_timer()\n",
    "    print(f\"time taken: {(end - start)/ 60} mins\")\n",
    "MSEs = np.array(MSEs)\n",
    "print(f\"MSE means: {np.mean(MSEs)}\")\n",
    "print(f\"MSE max: {np.max(MSEs)}\")\n",
    "print(f\"n_total: {MSEs.shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5741bccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Now let's graph the MSE distribution\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "n_total = MSEs.shape[0]\n",
    "mean_val = np.mean(MSEs)\n",
    "\n",
    "plt.hist(MSEs, density=True, bins=50, label=f\"Mean value: {mean_val}\\n max val outlier removed\") \n",
    "plt.ylabel('Occurrence')\n",
    "plt.xlabel('MSE');\n",
    "plt.title(f'MSE of hls vs torch prediction (n_total: {n_total})')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "plt.savefig('MSEs.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e149761",
   "metadata": {},
   "source": [
    "You can see from the graph above that the error is very small (order of magnitude -7). This will obviously get bigger once you use more realistic ap_fixed parameters, but this proves that the conversion itself is working as intended.\n",
    "\n",
    "So this is the latest progress on the pyg to hls conversion. The current model is only one layer out of eight pyg layers from the original Siqi's model. More work is on the way, but hopefully this gives you a good idea of how the conversion pipeline works. \n",
    "\n",
    "For any questions, please email me at yun@purdue.edu, or slack if you already have me on it.\n",
    "Thank you!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53405494",
   "metadata": {},
   "source": [
    "# Biography\n",
    "This walkthrough and other local files were taken from Mr Abd Elabd's code at https://github.com/abdelabd/manual_GNN_conversion <br />\n",
    "The hls4ml pyg support's starting code has been taken from Mr. Abd Elabd and Prof Javier Duarte's work: https://github.com/fastmachinelearning/hls4ml/tree/pyg_to_hls_rebase_w_dataflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d94335a2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
