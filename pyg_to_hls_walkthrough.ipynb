{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a1bc2776",
   "metadata": {},
   "source": [
    "### Installation\n",
    "The main ones are torch and hls4ml. Please install them using pip.\n",
    "\n",
    "If you still get import errors and pip doesn't tell you why, please have a look at environment.yml. \n",
    "\n",
    "It lists all the python packages that I have used to make this code work. Not all the packages in the yml file are necessary, however. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9ca91b5",
   "metadata": {},
   "source": [
    "### Imports\n",
    "\n",
    "The current hls4ml implementations does not support pytorch geometric(**pyg**). \n",
    "The starting code has been taken from Mr. Abd Elabd from Prof Javier Duarte's group. (Source: https://github.com/fastmachinelearning/hls4ml/tree/pyg_to_hls_rebase_w_dataflow)\n",
    "\n",
    "This means that once you \"pip install hls4ml\" to your conda env, you have to replace it with\n",
    "the local folder with the same name(\"hls4ml\"), which supports pyg. \n",
    "\n",
    "To find the conda env package location, simply do:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a9b384d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/swissman777/anaconda3/envs/hls4ml/lib/python3.7/site-packages/torch/__init__.py'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.__file__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5675568",
   "metadata": {},
   "source": [
    "The code above prints the location where my packages are installed. In my case, it was '/home/swissman777/anaconda3/envs/hls4ml/lib/python3.7/site-packages/torch/__init__.py', \n",
    "\n",
    "This means that my hls4ml package is located in '/home/swissman777/anaconda3/envs/hls4ml/lib/python3.7/site-packages/'\n",
    "\n",
    "Please go to the site-packages, find the directory \"hls4ml\", **delete** it, and **replace** it with the \"hls4ml\" file that we have in this repo. That is the package that supports pyg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9287a632",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "handler args: ('NodeBlock',)\n",
      "handler args: ('EdgeBlock',)\n",
      "handler args: ('EdgeAggregate',)\n",
      "handler args: ('ResidualBlock',)\n",
      "handler args: ('NodeEncoder',)\n",
      "handler args: ('EdgeEncoder',)\n",
      "handler args: ('NodeEncoderBatchNorm1d',)\n",
      "handler args: ('EdgeEncoderBatchNorm1d',)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "import rest of the packages\n",
    "\"\"\"\n",
    "import os\n",
    "import sys\n",
    "import yaml\n",
    "import argparse\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from hls4ml.utils.config import config_from_pyg_model\n",
    "from hls4ml.converters import convert_from_pyg_model\n",
    "\n",
    "\n",
    "from collections import OrderedDict\n",
    "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score, mean_absolute_error, mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "\n",
    "# locals\n",
    "from utils.models.interaction_network_pyg import InteractionNetwork\n",
    "from model_wrappers import model_wrapper\n",
    "from utils.data.dataset_pyg import GraphDataset\n",
    "from utils.data.fix_graph_size import fix_graph_size\n",
    "import time\n",
    "import pickle as pkl\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb118bac",
   "metadata": {},
   "source": [
    "### PyTorch Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "24082fea",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "We intialize our pyg model\n",
    "\"\"\"\n",
    "torch_model = InteractionNetwork(flow=\"source_to_target\", out_channels=128).eval() # eval mode for batchnorm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3bf92a2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "We obtain the state dict(trained parameters) from Siqi Miao, PhD student of Prof Pan Li\n",
    "\"\"\"\n",
    "state_dict = torch.load('./model.pt', map_location=\"cpu\")\n",
    "# state_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bc6ffa38",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "transfer the state_dict into our pyg model\n",
    "\"\"\"\n",
    "torch_model.beta = state_dict['model_state_dict']['convs.0.t']\n",
    "# print(type(torch_model.node_encoder.weight))\n",
    "# print(type(siqi_model_state_dict['model_state_dict']['node_encoder.weight']))\n",
    "torch_model.node_encoder.weight = nn.Parameter(state_dict['model_state_dict']['node_encoder.weight'])\n",
    "torch_model.node_encoder.bias = nn.Parameter(state_dict['model_state_dict']['node_encoder.bias'])\n",
    "torch_model.edge_encoder.weight = nn.Parameter(state_dict['model_state_dict']['edge_encoder.weight'])\n",
    "torch_model.edge_encoder.bias = nn.Parameter(state_dict['model_state_dict']['edge_encoder.bias'])\n",
    "torch_model.node_encoder_norm.weight = nn.Parameter(\n",
    "    state_dict['model_state_dict']['bn_node_feature.weight']\n",
    ")\n",
    "torch_model.node_encoder_norm.norm.weight = torch_model.node_encoder_norm.weight # this is temporary soln to the structure of the class\n",
    "\n",
    "torch_model.node_encoder_norm.bias = nn.Parameter(\n",
    "    state_dict['model_state_dict']['bn_node_feature.bias']\n",
    ")\n",
    "torch_model.node_encoder_norm.norm.bias = torch_model.node_encoder_norm.bias # this is temporary soln to the structure of the class\n",
    "\n",
    "torch_model.node_encoder_norm.running_mean = nn.Parameter(\n",
    "    state_dict['model_state_dict']['bn_node_feature.running_mean']\n",
    ")\n",
    "torch_model.node_encoder_norm.norm.running_mean = torch_model.node_encoder_norm.running_mean # this is temporary soln to the structure of the class\n",
    "\n",
    "torch_model.node_encoder_norm.running_var = nn.Parameter(\n",
    "    state_dict['model_state_dict']['bn_node_feature.running_var']\n",
    ")\n",
    "torch_model.node_encoder_norm.norm.running_var = torch_model.node_encoder_norm.running_var # this is temporary soln to the structure of the class\n",
    "\n",
    "\n",
    "torch_model.edge_encoder_norm.weight = nn.Parameter(\n",
    "    state_dict['model_state_dict']['bn_edge_feature.weight']\n",
    ")\n",
    "torch_model.edge_encoder_norm.norm.weight = torch_model.edge_encoder_norm.weight # this is temporary soln to the structure of the class\n",
    "\n",
    "torch_model.edge_encoder_norm.bias = nn.Parameter(\n",
    "    state_dict['model_state_dict']['bn_edge_feature.bias']\n",
    ")\n",
    "torch_model.edge_encoder_norm.norm.bias = torch_model.edge_encoder_norm.bias # this is temporary soln to the structure of the class\n",
    "\n",
    "torch_model.edge_encoder_norm.running_mean = nn.Parameter(\n",
    "    state_dict['model_state_dict']['bn_edge_feature.running_mean']\n",
    ")\n",
    "torch_model.edge_encoder_norm.norm.running_mean = torch_model.edge_encoder_norm.running_mean # this is temporary soln to the structure of the class\n",
    "\n",
    "torch_model.edge_encoder_norm.running_var = nn.Parameter(\n",
    "    state_dict['model_state_dict']['bn_edge_feature.running_var']\n",
    ")\n",
    "torch_model.edge_encoder_norm.norm.running_var = torch_model.edge_encoder_norm.running_var # this is temporary soln to the structure of the class\n",
    "\n",
    "mlp_name = \"mlps.0.\"\n",
    "original_layer_idxs = [0,1,4] # don't ask me why it jumps from 1 to 4\n",
    "new_layer_mlp_idxs = [0,1,3] # we skip 2 bc that's activation\n",
    "for idx in range(len(original_layer_idxs)):\n",
    "    original_layer_idx = original_layer_idxs[idx]\n",
    "    new_layer_mlp_idx = new_layer_mlp_idxs[idx]\n",
    "    module = torch_model.O.layers[new_layer_mlp_idx]\n",
    "    if (module.__class__.__name__ == 'Linear') or (module.__class__.__name__ == 'BatchNorm1d'):\n",
    "        module.weight = nn.Parameter(\n",
    "            state_dict['model_state_dict'][mlp_name+f\"{original_layer_idx}.weight\"]\n",
    "        )\n",
    "        module.bias = nn.Parameter(\n",
    "            state_dict['model_state_dict'][mlp_name+f\"{original_layer_idx}.bias\"]\n",
    "        )\n",
    "    if (module.__class__.__name__ == 'BatchNorm1d'):\n",
    "        module.running_mean = nn.Parameter(\n",
    "            state_dict['model_state_dict'][mlp_name+f\"{original_layer_idx}.running_mean\"]\n",
    "        )\n",
    "        module.running_var = nn.Parameter(\n",
    "            state_dict['model_state_dict'][mlp_name+f\"{original_layer_idx}.running_var\"]\n",
    "        )\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a122813a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weight loading for layer 0 successful: True\n",
      "bias loading for layer 0 successful: True\n",
      "weight loading for layer 1 successful: True\n",
      "bias loading for layer 1 successful: True\n",
      "weight loading for layer 2 successful: True\n",
      "bias loading for layer 2 successful: True\n",
      "running_mean loading for layer 1 successful: True\n",
      "running_var loading for layer 1 successful: True\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Just some code to test if the transfer was successful\n",
    "\"\"\"\n",
    "idxs = [0,1, 2]\n",
    "for idx in idxs:\n",
    "    original_layer_idx = original_layer_idxs[idx]\n",
    "    new_layer_mlp_idx = new_layer_mlp_idxs[idx]\n",
    "    boolean_val = torch.all(\n",
    "        torch_model.O.layers[new_layer_mlp_idx].state_dict()[\"weight\"] == state_dict['model_state_dict'][mlp_name+f\"{original_layer_idx}.weight\"]\n",
    "    )\n",
    "    print(f\"weight loading for layer {idx} successful: {boolean_val}\")\n",
    "\n",
    "\n",
    "    boolean_val = torch.all(\n",
    "        torch_model.O.layers[new_layer_mlp_idx].state_dict()[\"bias\"] == state_dict['model_state_dict'][mlp_name+f\"{original_layer_idx}.bias\"]\n",
    "    )\n",
    "    print(f\"bias loading for layer {idx} successful: {boolean_val}\")\n",
    "\n",
    "idx=1 # batchnorm layer idx. Batchnorm has additional parameters\n",
    "original_layer_idx = original_layer_idxs[idx]\n",
    "new_layer_mlp_idx = new_layer_mlp_idxs[idx]\n",
    "boolean_val = torch.all(\n",
    "    torch_model.O.layers[new_layer_mlp_idx].state_dict()[\"running_mean\"] == state_dict['model_state_dict'][mlp_name+f\"{original_layer_idx}.running_mean\"]\n",
    ")\n",
    "print(f\"running_mean loading for layer {idx} successful: {boolean_val}\")\n",
    "\n",
    "\n",
    "boolean_val = torch.all(\n",
    "    torch_model.O.layers[new_layer_mlp_idx].state_dict()[\"running_var\"] == state_dict['model_state_dict'][mlp_name+f\"{original_layer_idx}.running_var\"]\n",
    ")\n",
    "print(f\"running_var loading for layer {idx} successful: {boolean_val}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e49e754",
   "metadata": {},
   "source": [
    "### HLS Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76c70921",
   "metadata": {},
   "source": [
    "hls4ml cannot infer the *order* in which these submodules are called within the pytorch model's \"forward()\" function. We have to manually define this information in the form of an ordered-dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "18b64da5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "forward_dict: defines the order in which graph-blocks are called in the model's 'forward()' method\n",
    "\"\"\"\n",
    "forward_dict = OrderedDict()\n",
    "forward_dict[\"node_encoder\"] = \"NodeEncoder\"\n",
    "forward_dict[\"edge_encoder\"] = \"EdgeEncoder\"\n",
    "forward_dict[\"node_encoder_norm\"] = \"NodeEncoderBatchNorm1d\"\n",
    "forward_dict[\"edge_encoder_norm\"] = \"EdgeEncoderBatchNorm1d\"\n",
    "forward_dict[\"O\"] = \"NodeBlock\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c2a2df8",
   "metadata": {},
   "source": [
    "hls4ml creates a hardware implementation of the GNN, which can only be represented using fixed-size arrays. This restriction also applies to the inputs and outputs of the GNN, so we must define the size of the graphs that this hardware GNN can take as input**, again in the form of a dictionary. \n",
    "\n",
    "**Graphs of a different size can be padded or truncated to the appropriate size using the \"fix_graph_size\" function. In this notebook, padding/truncation is  done in the \"Data\" cell. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "baa5a9d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "we define additional parameters.\n",
    "\"\"\"\n",
    "common_dim = 128\n",
    "graph_dims = {\n",
    "        \"n_node\": 28,\n",
    "        \"n_edge\": 37,\n",
    "        \"node_attr\": 3,\n",
    "        \"node_dim\": common_dim,\n",
    "        \"edge_attr\": 4,\n",
    "    \"edge_dim\":common_dim\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51d5a2ab",
   "metadata": {},
   "source": [
    "Armed with our pytorch model and these two dictionaries**, we can create the HLS model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "59aa1957",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "config: {'Model': {'Precision': 'ap_fixed<52,20>', 'IndexPrecision': 'ap_uint<16>', 'ReuseFactor': 8, 'Strategy': 'Latency'}}\n",
      "Parsing Torch Layers into HLS ones\n",
      "Parsing Torch Layers into HLS ones\n",
      "Parsing Torch Layers into HLS ones\n",
      "Parsing Torch Layers into HLS ones\n",
      "Parsing Torch Layers into HLS ones\n",
      "Parsing Torch Layers into HLS ones\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "We initialize hls model from pyg model\n",
    "\"\"\"\n",
    "output_dir = \"test_GNN\"\n",
    "config = config_from_pyg_model(torch_model,\n",
    "                                   default_precision=\"ap_fixed<52,20>\",\n",
    "                                   default_index_precision='ap_uint<16>', \n",
    "                                   default_reuse_factor=8)\n",
    "print(f\"config: {config}\")\n",
    "hls_model = convert_from_pyg_model(torch_model,\n",
    "                                       n_edge=graph_dims['n_edge'],\n",
    "                                       n_node=graph_dims['n_node'],\n",
    "                                       edge_attr=graph_dims['edge_attr'],\n",
    "                                       node_attr=graph_dims['node_attr'],\n",
    "                                       edge_dim=graph_dims['edge_dim'],\n",
    "                                       node_dim=graph_dims['node_dim'],\n",
    "                                       forward_dictionary=forward_dict, \n",
    "                                       activate_final='sigmoid', #sigmoid\n",
    "                                       output_dir=output_dir,\n",
    "                                       hls_config=config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8f0701c",
   "metadata": {},
   "source": [
    "## hls_model.compile() builds the C-function for the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7da705cc",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing HLS project\n",
      "def_cpp: layer4_t layer4_out[N_LAYER_1_4*N_LAYER_2_4]\n",
      "def_cpp: layer5_t layer5_out[N_LAYER_1_5*N_LAYER_2_5]\n",
      "def_cpp: layer6_t layer6_out[N_LAYER_1_4*N_LAYER_2_4]\n",
      "def_cpp: layer7_t layer7_out[N_LAYER_1_5*N_LAYER_2_5]\n",
      "def_cpp: layer8_t layer8_out[N_NODE*LAYER8_OUT_DIM]\n",
      "def_cpp: layer9_t layer9_out[N_LAYER_1_4*LAYER9_OUT_DIM]\n",
      "Done\n",
      "lib_name: firmware/myproject-BA53F5bC.so\n"
     ]
    }
   ],
   "source": [
    "hls_model.compile()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e678b04",
   "metadata": {},
   "source": [
    "# Evaluation and prediction: hls_model.predict(input)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "e31a4c5a",
   "metadata": {},
   "source": [
    "If your model takes a non-singular input (e.g. node attributes, edge attributes, and an edge index), then you should pass it as a list (e.g. [node_attr, edge_attr, edge_index]). See the \"data_wrapper\" class, and note that the hls_model.predict() method is used on the data.hls_data attribute. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "661c3eaa",
   "metadata": {},
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "df4856a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IDS: [0 1 2 3 4 5 6 7 8 9]\n",
      "graphs length: 12\n",
      "writing test bench data for 1st graph\n"
     ]
    }
   ],
   "source": [
    "class data_wrapper(object):\n",
    "    def __init__(self, node_attr, edge_attr, edge_index, target):\n",
    "        self.x = node_attr\n",
    "        self.edge_attr = edge_attr\n",
    "        self.edge_index = edge_index.transpose(0,1)\n",
    "\n",
    "        node_attr, edge_attr, edge_index = self.x.detach().cpu().numpy(), self.edge_attr.detach().cpu().numpy(), self.edge_index.transpose(0, 1).detach().cpu().numpy().astype(np.float32)\n",
    "        node_attr, edge_attr, edge_index = np.ascontiguousarray(node_attr), np.ascontiguousarray(edge_attr), np.ascontiguousarray(edge_index)\n",
    "        self.hls_data = [node_attr, edge_attr, edge_index]\n",
    "\n",
    "        self.target = target\n",
    "        self.np_target = np.reshape(target.detach().cpu().numpy(), newshape=(target.shape[0],))\n",
    "\n",
    "def load_graphs(graph_indir, graph_dims, n_graphs):\n",
    "    graph_files = np.array(os.listdir(graph_indir))\n",
    "    graph_files = np.array([os.path.join(graph_indir, graph_file)\n",
    "                            for graph_file in graph_files])\n",
    "    n_graphs_total = len(graph_files)\n",
    "    IDs = np.arange(n_graphs_total)\n",
    "    print(f\"IDS: {IDs}\")\n",
    "    dataset = GraphDataset(graph_files=graph_files[IDs])\n",
    "\n",
    "    graphs = []\n",
    "    for data in dataset[:n_graphs]:\n",
    "        node_attr, edge_attr, edge_index, target, bad_graph = fix_graph_size(data.x, data.edge_attr, data.edge_index,\n",
    "                                                                             data.y,\n",
    "                                                                             n_node_max=graph_dims['n_node'],\n",
    "                                                                             n_edge_max=graph_dims['n_edge'])\n",
    "        if not bad_graph:\n",
    "            graphs.append(data_wrapper(node_attr, edge_attr, edge_index, target))\n",
    "        graphs.append(data_wrapper(node_attr, edge_attr, edge_index, target))\n",
    "    print(f\"graphs length: {len(graphs)}\")\n",
    "\n",
    "    print(\"writing test bench data for 1st graph\")\n",
    "    data = graphs[0]\n",
    "    node_attr, edge_attr, edge_index = data.x.detach().cpu().numpy(), data.edge_attr.detach().cpu().numpy(), data.edge_index.transpose(\n",
    "        0, 1).detach().cpu().numpy().astype(np.int32)\n",
    "    os.makedirs('tb_data', exist_ok=True)\n",
    "    input_data = np.concatenate([node_attr.reshape(1, -1), edge_attr.reshape(1, -1), edge_index.reshape(1, -1)], axis=1)\n",
    "    np.savetxt('tb_data/input_data.dat', input_data, fmt='%f', delimiter=' ')\n",
    "\n",
    "    return graphs\n",
    "\n",
    "\n",
    "graph_indir = \"trackml_data/processed_plus_pyg_small\"\n",
    "# graph_dims = {\n",
    "#         \"n_node\": 28,\n",
    "#         \"n_edge\": 37,\n",
    "#         \"node_dim\": 3,\n",
    "#         \"edge_dim\": 4\n",
    "#     }\n",
    "graphs = load_graphs(graph_indir, graph_dims, n_graphs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "318522c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE_l: [4.9447169e-07 4.9447169e-07 4.9626340e-07 5.4496633e-07 5.2348952e-07\n",
      " 5.2348952e-07 5.4247414e-07 4.8486396e-07 5.2672152e-07 5.4635973e-07\n",
      " 5.4945662e-07 5.4072916e-07]\n",
      "Mean of all MSEs: 5.223131438469863e-07\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Here we are testing hls model output compared to pyg model.\n",
    "We are using Mean Squared Error (MSE) to calculate the differences \n",
    "in the output of the two models.\n",
    "\"\"\"\n",
    "MSE_l = []\n",
    "for data in graphs:\n",
    "    torch_pred = torch_model(data)\n",
    "    torch_pred = torch_pred.detach().cpu().numpy().flatten()\n",
    "    hls_pred = hls_model.predict(data.hls_data)\n",
    "    MSE = mean_squared_error(torch_pred, hls_pred)\n",
    "    MSE_l.append(MSE)\n",
    "\n",
    "MSE_l = np.array(MSE_l)\n",
    "print(f\"MSE_l: {MSE_l}\")\n",
    "print(f\"Mean of all MSEs: {np.mean(MSE_l)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "23b1428f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSEs: \n",
      " [3.5399782e-07, 4.5376282e-07, 3.8164612e-07, 3.9868104e-07, 4.475846e-07, 3.782342e-07, 4.1409936e-07, 4.1074136e-07, 3.813671e-07, 4.398528e-07, 4.8113526e-07, 3.6632926e-07, 3.6684008e-07, 4.607302e-07, 4.543716e-07, 4.1573733e-07, 3.4768817e-07, 4.913622e-07, 3.751323e-07, 3.5784842e-07]\n"
     ]
    }
   ],
   "source": [
    "with open('test_data.pickle', 'rb') as f:\n",
    "    graphs= pkl.load(f) \n",
    "\n",
    "MSEs = []\n",
    "for data in graphs:\n",
    "    torch_pred = torch_model(data)\n",
    "    torch_pred = torch_pred.detach().cpu().numpy().flatten()\n",
    "    hls_pred = hls_model.predict(data.hls_data)\n",
    "    MSE = mean_squared_error(torch_pred, hls_pred)\n",
    "    MSEs.append(MSE)\n",
    "    \n",
    "print(f\"MSEs: \\n {MSEs}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62330a77",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "counter: 500\n",
      "time taken: 1.2573706366829962 mins\n",
      "counter: 500\n",
      "time taken: 1.2415757748337153 mins\n",
      "counter: 500\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Now let's load some of tau3mu data from our group (Prof Mia Liu).\n",
    "This is still a smaller sample of the total data, but it's good enough. \n",
    "\n",
    "NOTE: this will take some time (<5mins)\n",
    "\"\"\"\n",
    "import timeit\n",
    "\n",
    "MSEs = []\n",
    "stages = [\"train\", \"valid\", \"test\"]\n",
    "\n",
    "for stage in stages:\n",
    "    with open(f'tau3mu_data/test_BIG_data_{stage}.pickle', 'rb') as f:\n",
    "        graphs= pkl.load(f) \n",
    "        \n",
    "    counter = 0\n",
    "    start = timeit.default_timer()\n",
    "    for data in graphs:\n",
    "        # use counter to just keep track of the progress. Nothing fancy\n",
    "        if counter%500 ==0 and counter != 0:\n",
    "            print(f\"counter: {counter}\")\n",
    "        counter += 1\n",
    "        torch_pred = torch_model(data)\n",
    "        torch_pred = torch_pred.detach().cpu().numpy()\n",
    "        hls_pred = hls_model.predict(data.hls_data)\n",
    "        MSE = mean_squared_error(torch_pred, hls_pred)\n",
    "        MSEs.append(MSE)\n",
    "    end = timeit.default_timer()\n",
    "    print(f\"time taken: {(end - start)/ 60} mins\")\n",
    "MSEs = np.array(MSEs)\n",
    "print(f\"MSE means: {np.mean(MSEs)}\")\n",
    "print(f\"MSE max: {np.max(MSEs)}\")\n",
    "print(f\"n_total: {MSEs.shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5741bccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Now let's graph the MSE distribution\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "n_total = MSEs.shape[0]\n",
    "mean_val = np.mean(MSEs)\n",
    "\n",
    "plt.hist(MSEs, density=True, bins=50, label=f\"Mean value: {mean_val}\\n max val outlier removed\") \n",
    "plt.ylabel('Occurrence')\n",
    "plt.xlabel('MSE');\n",
    "plt.title(f'MSE of hls vs torch prediction (n_total: {n_total})')\n",
    "plt.legend()\n",
    "plt.savefig('MSEs.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "941a954a",
   "metadata": {},
   "source": [
    "You can see from the graph above that the error is very small (order of magnitude -7). This will obviously get bigger once you use more realistic ap_fixed parameters, but this proves that the conversion itself is working as intended.\n",
    "\n",
    "So this is the latest progress on the pyg to hls conversion. The current model is only one layer out of eight pyg layers from the original Siqi's model. More work is on the way, but hopefully this gives you a good idea of how the conversion pipeline works. \n",
    "\n",
    "For any questions, please email me at yun@purdue.edu, or slack if you already have me on it.\n",
    "Thank you!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53405494",
   "metadata": {},
   "source": [
    "Biography: This walkthrough and other local files were taken from Mr Abd Elabd's code at https://github.com/abdelabd/manual_GNN_conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b7b34b5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
